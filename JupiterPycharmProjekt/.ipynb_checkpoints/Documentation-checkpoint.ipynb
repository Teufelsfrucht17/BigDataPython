{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Big Data Project",
   "id": "43a749f873f2d9c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "2745eac91bbaa8ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import DataPrep\n",
    "import OLS_Regression\n",
    "import Ridge_Regression\n",
    "import SVR\n",
    "import NeuralNetworks\n",
    "import RandomForest\n",
    "import KNN\n",
    "\n",
    "print(DataPrep.report)\n",
    "import DataPrep\n",
    "\n",
    "import JupiterPycharmProjekt.OLS_Regression\n",
    "import JupiterPycharmProjekt.Ridge_Regression\n",
    "import JupiterPycharmProjekt.SVR\n",
    "import JupiterPycharmProjekt.NeuralNetworks\n",
    "import JupiterPycharmProjekt.RandomForest\n",
    "import JupiterPycharmProjekt.KNN\n",
    "\n",
    "print(DataPrep.report)"
   ],
   "id": "702367d3a0c6f4c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block executes all model training and evaluation scripts, collects their results and prints the final comparison report.",
   "id": "2f045ecbe28fe094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation  gerne nochmal checken glaube haben dort einiges verändert",
   "id": "4d8ae207ebdca07f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### General\n",
    "In this section the environment for Data Preparation is set up by importing essential Python libraries. Each library plays a key role for the Data Preparation."
   ],
   "id": "8055974a3d6549ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ],
   "id": "1899f155d4259a42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "pandas:             Used for handling and analyzing structured data\n",
    "\n",
    "matplotlib.pyplot:  A fundamental plotting library.\n",
    "\n",
    "seaborn:            Built on top of matplotlib and simplifies the process of graphical statistics.\n",
    "\n",
    "sklearn.model_selection + train_test_split: Helps to split a dataset into training and test dataset.\n",
    "\n",
    "sklearn.preprocessing + LabelEncoder: Transforming data before feeding it into a model.\n",
    "\n",
    "numpy: Is the foundational package for numerical computing in Python."
   ],
   "id": "e13ea412b5d512fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read original data from CSV",
   "id": "ce4cd1c0ed384672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = pd.read_csv('UsedCarSellingPrices.csv')",
   "id": "2047ddd961ca759"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code uses pandas to read the CSV file \"Used Car Selling Prices\" and loads it into a dataframe called 'data'.",
   "id": "1d92823130b25d88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label-Encoding for Visualisation before cleaning",
   "id": "adaa2653be36124d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define columns that need to be lable-encoded",
   "id": "636111fe5ed46561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']",
   "id": "a126c643f97ea4e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This line defines a list of column names that represent categorical features in the dataset.\n",
    "\n",
    "fuel: type of fuel that is used by the car\n",
    "\n",
    "seller_type: type of car seller\n",
    "\n",
    "transmission: type of gear\n",
    "\n",
    "owner: status of ownership"
   ],
   "id": "5ac874040e0796fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Copy data into var label_encoded_data; create empty array lable_encoders",
   "id": "211c5b124578ebcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T05:23:07.486025Z",
     "start_time": "2025-07-27T05:23:07.466696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_encoded_data = data.copy()\n",
    "label_encoders = {}"
   ],
   "id": "23182f4175e5701a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m label_encoded_data \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m      2\u001B[0m label_encoders \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code sets up the environment for label encoding.\n",
    "\n",
    "Line1: Copying the dataset\n",
    "\n",
    "Line2: Initializing the Encoders Dictionary"
   ],
   "id": "86d7636dd8db33ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run lable-encoder for every previosly defined columne (function imported from sklearn)",
   "id": "4a2da6e4f580939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T05:21:29.846703Z",
     "start_time": "2025-07-27T05:21:29.833995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    label_encoded_data[col] = le.fit_transform(label_encoded_data[col])\n",
    "    label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(\"\\nLabel-Encoded Data for Visualisation:\")\n",
    "print(label_encoded_data.head())"
   ],
   "id": "55ad639f84059f85",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_encoded_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m categorical_columns:\n\u001B[1;32m      3\u001B[0m     le \u001B[38;5;241m=\u001B[39m LabelEncoder()\n\u001B[0;32m----> 4\u001B[0m     label_encoded_data[col] \u001B[38;5;241m=\u001B[39m le\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mlabel_encoded_data\u001B[49m[col])\n\u001B[1;32m      5\u001B[0m     label_encoders[col] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(le\u001B[38;5;241m.\u001B[39mclasses_, le\u001B[38;5;241m.\u001B[39mtransform(le\u001B[38;5;241m.\u001B[39mclasses_)))\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mLabel-Encoded Data for Visualisation:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'label_encoded_data' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This loop iterates over each categorical column and applies Label Encoding transforming string labels into numeric codes.\n",
    "\n",
    "1. for col in categorical_columns: Loops through each column listed earlier\n",
    "2. le = labelEncoder(): Creates a new LabelEncoder instance from scikit-learn for the current column\n",
    "3. label_encoded_data[col] = le.fit_transform(label_encoded_data[col]): Fits the encoder to the column's categories and transforms them into integers + Replaces the original text values in label_encoded_data with the corresponding numeric labels\n",
    "4. label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_))): Stores the mapping of original category names to their encoded values in the label_encoders dictionary + This allows to trace or reverse the encoding later if needed"
   ],
   "id": "7dffe45da4a60e92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### concatinates x and y into one point to be visualized",
   "id": "97539411ff127a8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = pd.concat([label_encoded_data], axis=1)",
   "id": "2ca869967239fb39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new DataFrame called 'all_data_LableEncoded' by concatenating 'label_encoded_data' along the column axis 'axis=1'",
   "id": "537562c7d71a7d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Drops all columes that are non-numeric to make scaling possible",
   "id": "b54fe462afa4a1ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = all_data_LableEncoded.select_dtypes(include=['number'])",
   "id": "c30cefb4a2a48ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line filters the dataset to keep only the numeric columns from 'all_data_LableEncoded'.",
   "id": "568b8059b8e5d82a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scale data (normalized via MinMaxScaler - between 0 and 1)\n",
    "sscaler = preprocessing.StandardScaler()    ???\n",
    "\n",
    "all_data_LableEncoded = sscaler.fit_transform(all_data_LableEncoded)    ???"
   ],
   "id": "800483278abd6868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nscaler = preprocessing.MinMaxScaler()\n",
    "all_data_LableEncoded = nscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "bb45956ed8cc7525"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs Min-Max Scaling on the numeric features in the dataset, transforming them into a commonscale between 0 and 1.\n",
    "1. Line1: Initalizes a MinMaxScaler object from scikit-learn\n",
    "2. Line2: Calculates the min and max values for each feature iin the dataset and applies the scaling transformation to each value"
   ],
   "id": "51ed3765be9ad937"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisation before cleaning Data",
   "id": "318d63feac080e29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reintegrates Column name for boxplot",
   "id": "914c7df65d5ebf6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scaled_df = pd.DataFrame(all_data_LableEncoded, columns=label_encoded_data.select_dtypes(include='number').columns)",
   "id": "c0f59e3af51b2e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line converts the scaled NumPy array (from the Min-Max Scaler) back into a pandas DataFrame and restores the original column names.",
   "id": "6a3e41802100a48b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Boxplot with readable x-axis",
   "id": "ab1fa02b4c7ea8f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=scaled_df, orient='v', palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Normed boxplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "21635a8334e1f982"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot for each feature in the 'scaled_df' DataFrame to visualize the distribution and spread of the normalized (Min-max scaled) data.\n",
    "1. Line1: Sets the size of the figure to be 12 inches wide by 6 inches tall and ensures the plot is large enough to accommodate all features without crowding.\n",
    "2. Line2: Creates a vertical boxplot for each column in the 'scaled_df' DataFrame and uses Seaborn's elegant and color-friendly 'Set2' palette. Each box shows the median, the IQR and Whiskers & Outliers.\n",
    "3. Line3: Rotates the x-axis labels by 45 degrees for better readability, especially when there are many features.\n",
    "4. Line4: Adds a title to the plot for context, signaling that the data is normalized.\n",
    "5. Line5: Adjusts spacing to prevent overlap between axis labels, titles, and plot content.\n",
    "6. Line6: Renders and displays the final plot in the notebook."
   ],
   "id": "95cc2a4e33e73f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Boxplot for only numerical data",
   "id": "553d97e51b56ff55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_cols = ['selling_price', 'km_driven', 'year']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=scaled_df[selected_cols], orient='v', palette='Set3')\n",
    "plt.title(\"Normed boxplot for numerical data only\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c031fe8c9c522fee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This blocks generates a boxplot visualization focused on three specific, scaled numerical features:\n",
    "\n",
    "'selling_price'\n",
    "\n",
    "'km_driven'\n",
    "\n",
    "'year'\n",
    "\n",
    "1. Line1: Selects the subset of important numerical features for focused analysis.\n",
    "2. Line2: Sets the plot size to be 8 inches wide and 5 inches tall.\n",
    "3. Line3: Creates a vertical boxplot for just the selected columns using the soft, pastel 'Set3' color palette from Seaborn.\n",
    "4. Line4: Adds a descriptive title to clarify that this plot shows normalized (scaled) numerical features.\n",
    "5. Line5: Ensures layout is adjusted for neatness and then displays the plot."
   ],
   "id": "8ab645fd572c712c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pairplot to show correlation",
   "id": "2c73fec383780273"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.pairplot(scaled_df[selected_cols])\n",
    "plt.suptitle(\"Pairplot for select charactaristics\", y=1.02)\n",
    "plt.show()"
   ],
   "id": "7d6f172ff0f468e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This blocks creates a pairplot to visually explore pairwise relationships among the selected numerical features:\n",
    "\n",
    "'selling_price'\n",
    "\n",
    "'km_driven'\n",
    "\n",
    "'year'\n",
    "\n",
    "1. Line1: Creates a grid of scatterplots for each pairwise combination of the selected features. This helps to visualize Correlations, Clustering tendencies and Linearity or Non-Linearity Relationships. The Histograms are shown on the diagonal to represent each variable's distribution.\n",
    "2. Line2: Adds a super title above the entire plot grid.\n",
    "3. Line3: Renders the entire pairplot for viewing."
   ],
   "id": "20015cca9252c598"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean Data & Create variable Brand",
   "id": "40d32e9f2195e0f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove missing data",
   "id": "aa6db92fe290c3a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = data.dropna()",
   "id": "ca55134311fde7de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line removes all rows with missing values from the 'data' DataFrame.",
   "id": "137f5d67f47f07ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Show how much data was removed",
   "id": "453412919edc164b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Data after removing data:\")\n",
    "print(data.isnull().sum())\n",
    "print(f\"Remaining rows: {len(data)}\")"
   ],
   "id": "8bedb4c7d57d35f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block checks and confirms that all missing values have been removed from the dataset and reports the number of remaining rows.\n",
    "\n",
    "1. Line1: Prints a header to indicate that the following output relates to the cleaned dataset.\n",
    "2. Line2: Checks for missing values in each column of the 'data' DataFrame, creates a Boolean mask of the same shape as the data and then counts the number of 'True' values in each column, i.e. the number of missing entries.\n",
    "3. Line3: Prints the total number of rows left in the dataset after dropping rows with missing values using 'len(data)'."
   ],
   "id": "2ec06b73fc2937c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### IQR-based Removal of Outliers",
   "id": "15d94a870e94853d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = np.percentile(df[column], 25)\n",
    "    Q3 = np.percentile(df[column], 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]"
   ],
   "id": "4c2f625bd763d22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function removes outliers from a specific column in a DataFrame using the Interquartile Range (IQR) method\n",
    "\n",
    "1. 'Q1': 25th percentile - the value below which 25% of the data falls\n",
    "2. 'Q3': 75th percentile - the value below which 75% of the data falls\n",
    "3. 'IQR = Q3-Q1': IQR is the spread of the middle 50% of values and it is used to understand the natural range of variation in the data.\n",
    "4. Line5 + Line6: These define the acceptable range and any values below the lower bound or above the upper bound are considered outliers.\n",
    "5. Line7: Returns a filtered version of the original DataFrame, keeping only the rows where the specified column's value is within the acceptable range -> Outlier are removed."
   ],
   "id": "18841f9550170801"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Apply to most important column",
   "id": "a20f64473b0e5d0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = remove_outliers_iqr(data, 'selling_price')\n",
    "data = remove_outliers_iqr(data, 'km_driven')"
   ],
   "id": "3aca6d49a4bdbcc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These lines apply the IQR-based outlier removal function to two important columns in the dataset: 'selling_price' and 'km_driven'.\n",
    "\n",
    "1. Line1: Removes rows where 'selling_price' is considered an outlier based on the IQR rule and keeps only cars with selling prices within the typical range.\n",
    "2. Line2: Applies the same IQR filtering to the 'km_driven' column and eliminates unusually low or high mileage entries that could distort statistical analysis or model training."
   ],
   "id": "7b4af52698a24f65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nDaten nach IQR-basierter Ausreißerbereinigung:\")\n",
    "print(f\"Max. Verkaufspreis: {data['selling_price'].max()}\")\n",
    "print(f\"Max. Kilometerstand: {data['km_driven'].max()}\")\n",
    "print(f\"Verbleibende Zeilen nach IQR-Filter: {len(data)}\")"
   ],
   "id": "52ec2cf89495988b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block prints a quick summary of the dataset after removing outliers using the IQR method.\n",
    "1. Line1: Prints a headline used for clarity when reading the console output.\n",
    "2. Line2: Displays the maximum selling price in the cleaned dataset which helps verifying that extremely high prices have been removed.\n",
    "3. Line3: Shows the maximum odometer reading after outlier removal and ensures that unusally high mileage values have been filtered out.\n",
    "4. Line4: Prints the number of remaining rows in the dataset which tells how much data is left after removing rows that contained outliers in 'selling_price' and 'km_driven'."
   ],
   "id": "542b32c7b2a47b40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create 'Brand' as new column",
   "id": "4fd7b57da12e5d2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data['brand'] = data['name'].str.split().str[0]\n",
    "print(data)"
   ],
   "id": "24c854c0446ecc73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new column called 'brand' by extracting the first word from the 'name' column which represents the car brand.",
   "id": "bd2c2e327c96cbb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label-Encoding for Visualisation after cleaning",
   "id": "5b622991c547475f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ???",
   "id": "b0de47c75bf931c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\n",
    "label_encoded_data = data.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    label_encoded_data[col] = le.fit_transform(label_encoded_data[col])\n",
    "    label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(\"\\nLabel-Encoded Data (nur zur Referenz):\")\n",
    "print(label_encoded_data.head())"
   ],
   "id": "b992b00d1f2b76cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs label encoding on selected categorical columns, converting them from text values to integers so they can be used in regression models.\n",
    "\n",
    "1. Line1: Specifies the list of categorical features to be encoded which are typically textual descriptors that must be converted into numeric format for modeling.\n",
    "2. Line2: Creates a copy of the original dataset to apply the encodings without altering the raw data.\n",
    "3. Line3: Initializes an empty dictionary to store the encoding mappings for each categorical column.\n",
    "4. The 'for' loop: Iterates over each column in the 'categorical_columns' list; The 'LabelEncoder()' from scikit-learn is used to convert category labels into integers. The transformed values replace the original column in 'label_encoded_data'. The mapping of original class labels to integer codes is stored in 'label_encoders' for reference or inverse transformation later.\n",
    "5. Line10 + Line11: Displays the first few rows of the updated dataset to confirm that the categorical features have been encoded."
   ],
   "id": "ae11aef1a4a852dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### concatinate x and y into one point to be visualized",
   "id": "3d56e804c3a3f7e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = pd.concat([label_encoded_data], axis=1)",
   "id": "ac631a3c3f70be56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new DataFrame called 'all_data_LableEncoded' by concatenating the contents of 'label_encoded_data' along the columns axis.",
   "id": "a02b2277ecc55596"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Drop all columns that are non-numeric to make scaling possible",
   "id": "b799a1b7a761fd44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = all_data_LableEncoded.select_dtypes(include=['number'])",
   "id": "e876c32cc5528ba7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line filters the 'all_data_LableEncoded' DataFrame to include only numeric columns, removing any that are not numeric.",
   "id": "3b2f2c9998065419"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scale data (normalized)\n",
    "\n",
    "sscaler = preprocessing.StandardScaler()\n",
    "\n",
    "all_data_LableEncoded = sscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "87e0e4d63d317422"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nscaler = preprocessing.MinMaxScaler()\n",
    "all_data_LableEncoded = nscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "2778291857293e1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code applies Min-Max Scaling to normalize all numeric features in the dataset, transforming their values to a common range between 0 and 1.\n",
    "1. Line1: Initializes a MinMaxScaler object from scikit-learn's 'preprocessing' module and will scale each feature individually.\n",
    "2. Line2: Calculates the minimum and maximum values for each feature and scales each value in the dataset to the 0-1 range."
   ],
   "id": "db75c81582c47c0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization after Data cleaning",
   "id": "a5b963da4ad7e3fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scale Data ???",
   "id": "9b6560abdce3e4e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scaled_df = pd.DataFrame(all_data_LableEncoded, columns=label_encoded_data.select_dtypes(include='number').columns)",
   "id": "b06fcc296cd5e11d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line converts the scaled NumPy array back into a pandas DataFrame and restores the original column names, making the data human-readable and easier to work with.",
   "id": "99f0ed241a400120"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Boxplot with readable axis names",
   "id": "7ce6514781db7cf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=scaled_df, orient='v', palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot der skalierten numerischen Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "77115866ff932066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot for all features in the 'scaled_df' DataFrame, which contains only scaled numeric data. The plot helps visually inspecting the distribution and variability of each feature.\n",
    "1. Line1: Sets the figure size to 12 inches wide by 6 inches tall for better readability.\n",
    "2. Line2: Uses Seaborn to draw vertical boxplots for each numeric feature in 'scaled_df' and 'palette=Set2' gives the plot a soft, color-coded appearance to distinguish features visually.\n",
    "3. Line3: Rotates the x-axis labels by 45 degrees so that long feature names dont overlap and remain legible.\n",
    "4. Line4: Adds a descriptive title\n",
    "5. Line5 + Line6: Adjusts the layout to avoid overlapping elements and displays the final plot."
   ],
   "id": "7ec0aa4417ca0591"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Boxplot only for selected numeric columns",
   "id": "9849f41d1ddf0c6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_cols = ['selling_price', 'km_driven', 'year']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=scaled_df[selected_cols], orient='v', palette='Set3')\n",
    "plt.title(\"Boxplot ausgewählter Merkmale\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f5c70ed402522bd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot visualization for a selected subset of key numeric features: 'selling_price', 'km_driven' and 'year', all of which have been previously scaled to 0-1 range.\n",
    "1. Line1: Selects the three features for targeted visualization.\n",
    "2. Line2: Sets the figure size to 8 inches wide and 5 inches tall for compact clarity.\n",
    "3. Line3: Draws vertical boxplots for just the selected columns using Seaborn's pastel 'Set3' color palette.\n",
    "4. Line4: Adds a title.\n",
    "5. Line5 + Line6: Adjusts spacing to prevent overlap and displays the plot."
   ],
   "id": "3412ee8bd6ffa66a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Pairplot for Distribution and Correlation",
   "id": "8dde95221dbd9000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.pairplot(scaled_df[selected_cols])\n",
    "plt.suptitle(\"Paarweise Verteilungen ausgewählter Merkmale\", y=1.02)\n",
    "plt.show()\n",
    "#print(\"Trainingsdaten zusätzlich als 'prepared_used_car_data_train.parquet' gespeichert.\")\n",
    "#print(\"Testdaten zusätzlich als 'prepared_used_car_data_test.parquet' gespeichert.\")\n",
    "#print(\"Gesamtdaten zusätzlich als 'prepared_used_car_data.parquet' gespeichert.\")"
   ],
   "id": "d4cddbba60eae9df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a pairplot that visualizes the pairwise relationships and distributions of three selected, scaled features: 'selling_price', 'km_driven' and 'year'.\n",
    "1. Line1: Generates a grid of plots: Scatter plots and Histograms.\n",
    "2. Line2: Adds a descriptive title above the plot grid; 'y=1.02' adjusts the title position slightly above the plot area to prevent overlap.\n",
    "3. Line3: Renders and displays the plot."
   ],
   "id": "ff8b32c1a9301931"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### One-Hot-Encoding for Regression Model Training & Testing",
   "id": "7afb4ecd9e7f6f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### One-Hot-Encoding for categorical Variables",
   "id": "412c72b4510faa63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoded_data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(\"\\nOne-Hot-Encoded Data:\")\n",
    "print(encoded_data.head())\n",
    "\n",
    "print(encoded_data)"
   ],
   "id": "a6f7e24bde3e3f2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block uses one-hot encoding to transform categorical columns in the dataset into binary 0 or 1 columns, making them suitable for regression models.\n",
    "1. Line1: Performs one-hot encoding on the columns listed in 'categorical_columns'. For each unique category in these columns, new binary columns are created. Drops the first category for each column to avoid multicollinearity when using models like linear regression.\n",
    "2. Line3: Prints a header for clarity in console output.\n",
    "3. Line4: Displays the first few rows of the encoded dataset for a quick preview.\n",
    "4. Line6: Prints the entire DataFrame, which now contains both numeric and one-hot encoded binary columns."
   ],
   "id": "bf53c95f9b307d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sort data by 'year' and 'km_driven'",
   "id": "b882593da83debe1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "encoded_data_sorted = encoded_data.sort_values(by=['year', 'km_driven'], ascending=[False, True])",
   "id": "2629b0bf781225dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line sorts the encoded dataset based on two columns - 'year' and 'km_driven' - to organize the data in a meaningful order.",
   "id": "272c5b9fd499ac84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare data used for regression analysis",
   "id": "a8d10f2a611467bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features = encoded_data_sorted.columns.drop(['name', 'selling_price'])\n",
    "target = 'selling_price'\n",
    "\n",
    "X = encoded_data_sorted[features]\n",
    "y = encoded_data_sorted[target]"
   ],
   "id": "143bc2245e4d47d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block prepares the feature matrix 'X' and target vector 'Y' for training a regression model, using sorted and one-hot encoded dataset.\n",
    "1. Line1: Selects all column names except: 'name' and 'selling_price' -> Result is a list of input features for the model.\n",
    "2. Line2: Explicitly defines 'selling_price' as the target variable.\n",
    "3. Line4: Creates the feature matrix 'X' by selecting only the columns in 'features' from the dataset; 'X' will be used as input for the regression model\n",
    "4. Line5: Creates the target vector 'Y', which contains the selling prices (values to be predicted)"
   ],
   "id": "c8fb5138c428fba7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving test data",
   "id": "a5ad4cfd3b676531"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_data = pd.concat([X, y], axis=1)\n",
    "all_data.to_csv('prepared_used_car_data_all.csv', index=False)"
   ],
   "id": "dd2d0576a6403f24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block recombines the feature 'X' and target 'Y' into a single DataFrame and saves it as a '.csv' file for future use.\n",
    "1. Line1: Concatenates the feature matrix 'X' and the target vector 'Y' horizontally and reconstructs the full dataset 'all_data' with both inputs and outputs in one table.\n",
    "2. Line2: Saves the combined dataset to a CSV file and ensures that the DataFrame index is not written to the file, keeping the output clean and suitable for reuse."
   ],
   "id": "ec434fed0a927f84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creation of training and test data",
   "id": "9295e915558baf12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "id": "5339af0b365f5a0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line uses scikit-learn's 'train_test_split()' function to divide the dataset into training and testing subsets, a crucial step for evaluating regression models.",
   "id": "ac57a39dbe4f2d0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving of training data",
   "id": "787270178633509a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data.to_csv('prepared_used_car_data_train.csv', index=False)"
   ],
   "id": "a1fe4ed740f72b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block combines the training features and labels into a single DataFrame and then exports it to a '.csv' file for storage or reuse.\n",
    "1. Line1: Merges the training input features 'X_train' and then training target values 'Y_train' side by side (along columns) and produces a single DataFrame 'train_data' that contains all the necessary data for training a model.\n",
    "2. Line2: Saves the 'train_data' DataFrame as a CSV file and ensures the row indices are not written into the file, keeping it clean and easy to reload."
   ],
   "id": "6c121e4e808da1e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving of test data",
   "id": "b48b9753e3c5bed5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data.to_csv('prepared_used_car_data_test.csv', index=False)"
   ],
   "id": "71fd609e140c0831"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code combines the test features and labels into a single DataFrame and then saves it as a CSV file for future use or evaluation.\n",
    "1. Line1: Merges the test feature set 'X_test' and the corresponding target values 'Y_test' horizontally and produces a new DataFrame 'test_data' that includes all the columns needed to evaluate a regression model.\n",
    "2. Line2: Saves the resulting test dataset to a CSV file and prevents the row index from being included in the file, making the CSV clean and readable."
   ],
   "id": "d7b18279d9abcd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confirming saved files",
   "id": "340e500960290d4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nTrainingsdaten gespeichert als 'prepared_used_car_data_train.csv'\")\n",
    "print(\"Testdaten gespeichert als 'prepared_used_car_data_test.csv'\")"
   ],
   "id": "79d07b7563dceaab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These print statements simply confirm to the user that the training and test datasets have been successfully saved to CSV files.\n",
    "1. Line1: Outputs a message confirming that the training data was saved.\n",
    "2. Line2: Confirms that the test data was also saved."
   ],
   "id": "5bf4c8dfd38c977d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KNN",
   "id": "6d0fecab02868e3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Set Up",
   "id": "a279dafcacd6108a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import DataPrep\n",
    "\n",
    "(X_train_KNN, X_test_KNN, Y_train_KNN, Y_test_KNN) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)"
   ],
   "id": "8ab5c5127d0f444"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block sets upo the environment to train and evaluate a K-Nearest Neighbors Regression model using a train/test split of preprocessed data.\n",
    "\n",
    "1. Line1: Imports the KNN regressor from scikit-learn\n",
    "2. Line2: Adds a tool for hyperparameter tuning by trying different values and splits the dataset into training and testing sets\n",
    "3. Line3: Imports the DataPrep file\n",
    "4. Line5: Splits features and target into a training set and testing set while ensuring that the split is reproducible"
   ],
   "id": "2515df4caecbccb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Initialize KNN model",
   "id": "ace51a6848a1c3e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "knnmodelCV = KNeighborsRegressor()",
   "id": "88badaf488edc889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates an instance of the KNN model from scikit-learn with default parameters.",
   "id": "235ac04d364a14c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define grid of neighbor counts",
   "id": "181fc54b35f9c218"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "'n_neighbors': range(3, 22, 2),\n",
    "'weights': ['uniform', 'distance'],\n",
    "'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "'leaf_size': [8, 16, 32, 64, 128, 256, 512],\n",
    "'p': [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "}"
   ],
   "id": "9592a6c38721f984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block defines a hyperparameter grid that will be used to tune a KNN regressor using tools like GridSearchCV.\n",
    "\n",
    "1. Line2: Tries odd values for k from 3 to 21 which defines how many neighbors the model considers when making predictions.\n",
    "2. Line3: 'uniform' makes sure all neighbors contribute equally to the prediction and 'distance' makes sure that closer neighbors contribute more to the prediction than distant ones.\n",
    "3. Line4: Specifies the search algorithm used to find neighbors. 'auto' -> Chooses the best algorithm based on the data. 'ball_tree' and 'kd_tree' -> Use tree structure for fast searches. 'brute' -> Calculates distances directly.\n",
    "4. Line5: Affects the speed vs. memory tradeoff in tree based algorithms. Smaller values result in faster query time. Larger values result in faster tree building.\n",
    "5. Line6: Specifies the power parameter for the Minkowski distance."
   ],
   "id": "b52981a9a23aff06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run 10-fold cross-validation across neighbor settings",
   "id": "b26d0ba7143d7b49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_knnmodel = GridSearchCV(estimator=knnmodelCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_knnmodel.fit(X_train_KNN, Y_train_KNN)"
   ],
   "id": "fc08102981b6da34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code uses GridSearchCV from scikit-learn to perform an exhaustive search over a defined set of hyperparameters for a KNN regression model.\n",
    "Assigns all available CPU cores to parallelize the search and speed up computation and uses 10-fold cross-validation."
   ],
   "id": "988968524d60c9f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output the best number of neighbors",
   "id": "d5c28544ac4ee4c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Best parameters found:\", CV_knnmodel.best_params_)",
   "id": "f6eff7ba01fea793"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line prints the optimal set of hyperparameters that were found during the GridSearchCV process.",
   "id": "67f144b89d5abe4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating Model Performance",
   "id": "4db8547adf6044f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_knnmodel.predict(X_train_KNN)\n",
    "Y_train_dev = sum((Y_train_KNN - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_KNN - Y_train_KNN.mean())**2)\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev"
   ],
   "id": "5160858f1255c7ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually calculates the coefficient of determination (r2) to evaluate how well the KNN model fits the training data.\n",
    "1. Line1: Uses the trained cross-validated KNN model to predict selling prices for the training set.\n",
    "2. Line2: Calculates the residual sum of squares (RSS) and measures how far off the predictions are from the actual values which should be minimal.\n",
    "3. Line3: Calculates the total sum of squares (TSS) and measures the total variation in the target variable (how far are values from the mean)\n",
    "4. Line4: Calculates the coefficient of determination (r2)."
   ],
   "id": "f432076650e8b79c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict on test set",
   "id": "7a22ad0b3b7350b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_knnmodel.predict(X_test_KNN)\n",
    "Y_test_dev = sum((Y_test_KNN - Y_test_pred)**2)\n",
    "Y_test_meandev = sum((Y_test_KNN - Y_test_KNN.mean())**2)\n",
    "pseudor2 = 1 - Y_test_dev / Y_test_meandev\n",
    "\n",
    "DataPrep.report.loc[len(DataPrep.report)] = [\"KNN_LE \", r2, pseudor2,\"\", CV_knnmodel.cv_results_['mean_test_score'][CV_knnmodel.best_index_], CV_knnmodel.cv_results_['std_test_score'][CV_knnmodel.best_index_]]"
   ],
   "id": "3fedff4da87adf0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block evaluates the KNN model on the test set using the coefficient of determination and calculates a pseudo r2 score for generalization performance and appends a summary row to report DataFrame for later comparison with other models.\n",
    "1. Line1: Uses the best tuned KNN model to predict the selling prices for the test set.\n",
    "2. Line2: Calculates the residual sum of squares (RSS) on the test data.\n",
    "3. Line3: Computes the total sum of squares (TSS) of the actual test values\n",
    "4. Line4: Calculates pseudo-r2 which is the models explanatory power on the unseen test data.\n",
    "5. Line6: Logs results to a report."
   ],
   "id": "9bcdf9e0b6a1dda0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Encoded Data for Training and Testing",
   "id": "30d26874ff324b8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train_KNN, X_test_KNN, Y_train_KNN, Y_test_KNN) = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "4cfb81bd9d65c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the one-hot encoded dataset into training and testing sets for building and evaluating a KNN model.",
   "id": "2c1e6bb49a11d4be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run 10-fold cross-validation across neighbor settings",
   "id": "910bb56d32550919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_knnmodel = GridSearchCV(estimator=knnmodelCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_knnmodel.fit(X_train_KNN, Y_train_KNN)"
   ],
   "id": "feb5f84a3e8f61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs hyperparameter optimization for the KNN regression model using one-hot encoded data with cross validation to ensure reliable model selection.\n",
    "1. Line1: Performs an exhaustive grid search over the parameter combinations defined in 'param_grid'.\n",
    "2. Line2: Trains and evaluates all hyperparameter combinations using the training data based on one-hot encoded features."
   ],
   "id": "9409cbb861e863af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output the best number of neighbors",
   "id": "3e25360e653fa41e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Best parameters found:\", CV_knnmodel.best_params_)\n",
    "Y_train_pred = CV_knnmodel.predict(X_train_KNN)\n",
    "Y_train_dev = sum((Y_train_KNN - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_KNN - Y_train_KNN.mean())**2)\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev"
   ],
   "id": "3e859ee0673cef0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This line prints the optimal hyperparameter combination selected by 'GridSearchCV' for the KNN regression model trained on one-hot encoded data.\n",
    "1. Line2: Predicts the selling prices for the training data using the best-found KNN model.\n",
    "2. Line3: Computes the residual sum of squares (RSS) and measures how much error remains after using the models predictions.\n",
    "3. Line4: Computes the total sum of squares (TSS) and represents the total variance in the training labels serving as a baseline.\n",
    "4. Line5: Calculates the r2 score which explains how much variation is explained by the model."
   ],
   "id": "9e431f847130f0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict on test set",
   "id": "cc5c7483f12929e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_knnmodel.predict(X_test_KNN)\n",
    "Y_test_dev = sum((Y_test_KNN - Y_test_pred)**2)\n",
    "Y_test_meandev = sum((Y_test_KNN - Y_test_KNN.mean())**2)\n",
    "pseudor2 = 1 - Y_test_dev / Y_test_meandev"
   ],
   "id": "80df821b58a8bb5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block evaluates how well the tuned KNN model generalizes to unseen data by manually calculating the pseudo r2 score.\n",
    "1. Line1: Uses the trained KNN model to predict selling prices for the test set.\n",
    "2. Line2: Computes the residual sum of squares (RSS) for the test set and measures the models prediction error on unseen data.\n",
    "3. Line3: Compues the total sum of squares (TSS) for the test set and represents the total variance in the actual test values serving as a baseline.\n",
    "4. Line4: Calculates the pseudo-r2 and indicates how well the model generalizes to new unseen data."
   ],
   "id": "39dd25bf3d000228"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Log KNN Model results in a report",
   "id": "198855f194f5b100"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DataPrep.report.loc[len(DataPrep.report)] = [\"KNN_OH \", r2, pseudor2,\"\", CV_knnmodel.cv_results_['mean_test_score'][CV_knnmodel.best_index_], CV_knnmodel.cv_results_['std_test_score'][CV_knnmodel.best_index_]]\n",
    "print(DataPrep.report.head())\n",
    "print(CV_knnmodel.best_params_)"
   ],
   "id": "862aaaf61a881b3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block logs the performance metrics of the KNN model trained with one-hot encoded data into a centralized evaluation report and then displays the report and the best hyperparameters.",
   "id": "477529706c558ea5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neural Networks",
   "id": "277f35199f8a5b8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preparation to train a Neural Network",
   "id": "9fd038aed58feaba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import DataPrep"
   ],
   "id": "255b5cd21368e7cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block sets up the environment for training and tuning a neural network regression model using scikit-learn's 'MLPRegressor' with data and configuration coming from a shared module called 'DataPrep'.\n",
    "1. Line1: Used to split data into training and testing sets and enables hyperparameter tuning using exhaustive search with cross-validation.\n",
    "2. Line2: Imports 'MLPRegressor' a multilayer perceptron for regression tasks.\n",
    "3. Line3: Imports the 'DataPrep' module."
   ],
   "id": "2b20afb0dd1d63ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting Label-Encoded Data",
   "id": "fd142d44b58040cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_nn, X_test_nn, Y_train_nn, Y_test_nn) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)",
   "id": "8b76ee4828bcaaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the label-encoded dataset into training and testing sets preparing it for use in training a neural network regression model.",
   "id": "b062b2e50ce03f9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining a Hyperparameter Grid for Tuning the Network Regressor",
   "id": "74d365cd1c9e1c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T18:20:38.125805Z",
     "start_time": "2025-07-27T18:20:38.121102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "'hidden_layer_sizes': [(5,), (8,), (10,), (13,)],\n",
    "'alpha': [0.0, 0.0025, 0.005, 0.0075, 0.01, 0.1],\n",
    "'activation': ['logistic', 'tanh', 'relu'],\n",
    "'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "'max_iter': [5000],\n",
    "'random_state': [0],\n",
    "'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "}"
   ],
   "id": "d12f18255c66c8cb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block defines a parameter grid to be used with 'GridSearchCV' for tuning a 'MLPRegressor' model in a regression task. Each key represents a hyperparameter and each value is a list of options that 'GridSearchCV' will explore during cross-validation.\n",
    "1. Line2: Defines the structure of the hidden layers; how many neurons\n",
    "2. Line3: L2 regularization term which helps to prevent overfitting by penalizing large weights\n",
    "3. Line4: Specifies the activation function used in the hidden layers\n",
    "4. Line5: Optimization algorithm used for training\n",
    "5. Line6: Sets the maximum number of training iterations to 5000\n",
    "6. Line7: Ensures reproducibility by fixing the random seed used inernally by the model\n",
    "6. Line8: Controls how the learning rate adapts over time when using 'sgd' or 'adam'"
   ],
   "id": "3db493682d99c3e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training a Neural Network with Grid Search and Cross-Validation",
   "id": "e2740a79b2944c80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NNetRregCV = MLPRegressor()\n",
    "CV_nnmodel = GridSearchCV(estimator=NNetRregCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_nnmodel.fit(X_train_nn, Y_train_nn)"
   ],
   "id": "7268b1c7c30eb5f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates, tunes and trains a neural network regressor using scikit-learn's 'MLPRegressor' and 'GridSearchCV'.\n",
    "1. Line1: Initializes a Multi-Layer Perceptron (MLP) Regressor which is a type of feedforward neural network.\n",
    "2. Line2: Wraps the neural network model with 'GridSearchCV' to perform automated hyperparameter tuning.\n",
    "3. Line3: Trains the model using the training data and runs the grid search."
   ],
   "id": "d75be97d98806a6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the Neural Network on Training Data",
   "id": "e251e56bd7a93d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_nnmodel.predict(X_train_nn)\n",
    "Y_train_dev = sum((Y_train_nn - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_nn - Y_train_nn.mean())**2)  # [aus PDF]\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev  # [aus PDF]"
   ],
   "id": "cf062e72060771d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually calculates the r2 score to evaluate how well the trained neural network model fits the training data.\n",
    "1. Line1: Uses the best neural network model selected by 'GridSearchCV' to predict target values for the training set\n",
    "2. Line2: Calculates the residual sum of squares (RSS) and measures the total prediction error of the model\n",
    "3. Line3: Calculates the total sum of squares (TSS) and measures how much variance is in the target data without any model\n",
    "4. Line4: Computes the r2 score which explains the variance in the training data"
   ],
   "id": "ba4d068a94eaf601"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the Neural Network on Test Data",
   "id": "8bcbeea1ddaef52a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_nnmodel.predict(X_test_nn)\n",
    "Y_test_dev = sum((Y_test_nn - Y_test_pred)**2)\n",
    "Y_train_meandev = sum((Y_test_nn - Y_test_nn.mean())**2)  # [aus PDF]\n",
    "pseudor2 = 1 - Y_test_dev / Y_train_meandev"
   ],
   "id": "1d9f880376cc9ad1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block calculates the pseudo r2 score to assess how well the trained neural network regressor performs on unseen test data.\n",
    "1. Line1: Predicts target values for the test set using the neural network model selected by 'GridSearchCV'\n",
    "2. Line2: Computes the residual sum of squares (RSS) for the test set which represents the total prediction error of the model on unseen data\n",
    "3. Line3: Computes the total sum of squares (TSS) for the test set which represents the total variance in the target values\n",
    "4. Line4: Calculates the pseudo r2 score used to evaluate the model performance on the test data by measuring how much variability in the test target data is explained by the model"
   ],
   "id": "45571324c387e8c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logging Neural Network Results into the Model Comparison Report",
   "id": "20103da1d8ba9a37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DataPrep.report.loc[len(DataPrep.report)] = [\"NN_LE \", r2, pseudor2,\"\", CV_nnmodel.cv_results_['mean_test_score'][CV_nnmodel.best_index_], CV_nnmodel.cv_results_['std_test_score'][CV_nnmodel.best_index_]]",
   "id": "d28c70c6711c93c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line adds a new row to the shared 'DataPrep.report' table documenting the performance metrics of the neural network model trained with label-encoded data.",
   "id": "b67ff470c5a202f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Displaying the best hyperparameters",
   "id": "2205eaad77c67648"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(CV_nnmodel.best_params_)",
   "id": "e47716d0d39e188b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line prints the optimal hyperparameter configuration found by 'GridSearchCV' during training of the MLPRegressor",
   "id": "910d0ae5a528d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Encoded Data for Neural Network Training and Testing",
   "id": "ad45d225bb72e95d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_nn, X_test_nn, Y_train_nn, Y_test_nn) = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "b93b793d472b79f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the dataset - specifically the one-hot encoded version - into a training set and a test set preparing it for use with a neural network regression model",
   "id": "e1d6e311087bbef4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Grid Search to Train and Tune a Neural Network with One-Hot Encoded Data",
   "id": "eee234ef036dcaae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_nnmodel = GridSearchCV(estimator=NNetRregCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_nnmodel.fit(X_train_nn, Y_train_nn)"
   ],
   "id": "3d2731f814816b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code performs automated hyperparameter tuning using 'GridSearchCV' for a neural network regression model trained on one-hot encoded features.\n",
    "1. Line1: Wraps the 'MLPRegressor' in a 'GridSearchCV' object to perform an exhaustive search over a grid of hyperparameters\n",
    "2. Line2: Trains the model using the training data"
   ],
   "id": "56cd1a1a563800ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculating r2 score for Neural Network",
   "id": "fc00be5abb2b1e32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_nnmodel.predict(X_train_nn)\n",
    "Y_train_dev = sum((Y_train_nn - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_nn - Y_train_nn.mean())**2)  # [aus PDF]\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev  # [aus PDF]"
   ],
   "id": "a15a7cb1842002c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually computes the r2 score for the neural network model trained on one-hot encoded features using the training dataset\n",
    "1. Line1: Predicts the target values on the training data using the best neural network selected by 'GridSearchCV'\n",
    "2. Line2: Calculates the residual sum of squares (RSS) which represents the total prediction error of the model on the training set.\n",
    "3. Line3: Calculates the total sum of squares (TSS) which represents the total variance in the training data assuming a baseline model that always predicts the mean\n",
    "4. Line4: Computes the r2 score which measures how well the model explains the variance in the target variable"
   ],
   "id": "e2058ea211163e74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculating pseudo r2 score for Neural Network",
   "id": "bb17134b806cb222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_nnmodel.predict(X_test_nn)\n",
    "Y_test_dev = sum((Y_test_nn - Y_test_pred)**2)\n",
    "Y_train_meandev = sum((Y_test_nn - Y_test_nn.mean())**2)  # [aus PDF]\n",
    "pseudor2 = 1 - Y_test_dev / Y_train_meandev"
   ],
   "id": "7bff458aa6fa79b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code block calculates the r2 score on the test dataset (pseudo r2) to evaluate the generalization performance of the neural network trained on one-hot encoded features.\n",
    "1. Line1: Uses the trained neural network to predict values for the test set.\n",
    "2. Line2: Computes the residual sum of squares (RSS) on the test set and measures the total prediction error made by the model on unseen data\n",
    "3. Line3: Calculates the total sum of squares (TSS) on the test data\n",
    "4. Line4: Calculates the test r2 (pseudo r2) which reflects how well the model generalizes to new data"
   ],
   "id": "de1b08898d417708"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logging Neural Network Results to the Report",
   "id": "49dd46b2fb7bb15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DataPrep.report.loc[len(DataPrep.report)] = [\"NN_OH \", r2, pseudor2,\"\", CV_nnmodel.cv_results_['mean_test_score'][CV_nnmodel.best_index_], CV_nnmodel.cv_results_['std_test_score'][CV_nnmodel.best_index_]]\n",
    "print(DataPrep.report.head())\n",
    "print(CV_nnmodel.best_params_)"
   ],
   "id": "9f2ab62944c33db9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block adds a new row to your 'DataPrep.report' table to document the performance of a neural network model trained on one-hot encoded features and then displays the first few rows of the report and the best hyperparameters found during grid search.",
   "id": "e60fb7e259a8aa14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
