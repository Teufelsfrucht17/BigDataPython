{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Big Data Project",
   "id": "1bf3680e1a6df4c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "64dabb562aa9b3a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import DataPrep\n",
    "import OLS_Regression\n",
    "import Ridge_Regression\n",
    "import SVR\n",
    "import NeuralNetworks\n",
    "import RandomForest\n",
    "import KNN\n",
    "\n",
    "print(DataPrep.report)\n",
    "import DataPrep\n",
    "\n",
    "import JupiterPycharmProjekt.OLS_Regression\n",
    "import JupiterPycharmProjekt.Ridge_Regression\n",
    "import JupiterPycharmProjekt.SVR\n",
    "import JupiterPycharmProjekt.NeuralNetworks\n",
    "import JupiterPycharmProjekt.RandomForest\n",
    "import JupiterPycharmProjekt.KNN\n",
    "\n",
    "print(DataPrep.report)"
   ],
   "id": "4aa6727a8fa07795"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block executes all model training and evaluation scripts, collects their results and prints the final comparison report.",
   "id": "e442bc24151843d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation  gerne nochmal checken glaube haben dort einiges ver√§ndert",
   "id": "b75501c1bdf61f70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### General\n",
    "In this section the environment for Data Preparation is set up by importing essential Python libraries. Each library plays a key role for the Data Preparation."
   ],
   "id": "f23b2b11019ed5c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ],
   "id": "3c8972a9db816a0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "pandas:             Used for handling and analyzing structured data\n",
    "\n",
    "matplotlib.pyplot:  A fundamental plotting library.\n",
    "\n",
    "seaborn:            Built on top of matplotlib and simplifies the process of graphical statistics.\n",
    "\n",
    "sklearn.model_selection + train_test_split: Helps to split a dataset into training and test dataset.\n",
    "\n",
    "sklearn.preprocessing + LabelEncoder: Transforming data before feeding it into a model.\n",
    "\n",
    "numpy: Is the foundational package for numerical computing in Python."
   ],
   "id": "a05def101079fdb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read original data from CSV",
   "id": "80b03ee3728e4444"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = pd.read_csv('UsedCarSellingPrices.csv')",
   "id": "280e7d47459c1d19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code uses pandas to read the CSV file \"Used Car Selling Prices\" and loads it into a dataframe called 'data'.",
   "id": "d1275f6a59584826"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label-Encoding for Visualisation before cleaning",
   "id": "3ca57ba84a5a98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define columns that need to be lable-encoded",
   "id": "a9e48d7f80daa9de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']",
   "id": "647a721af1a27119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This line defines a list of column names that represent categorical features in the dataset.\n",
    "\n",
    "fuel: type of fuel that is used by the car\n",
    "\n",
    "seller_type: type of car seller\n",
    "\n",
    "transmission: type of gear\n",
    "\n",
    "owner: status of ownership"
   ],
   "id": "f9a160792dae95fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Copy data into var label_encoded_data; create empty array lable_encoders",
   "id": "f0568723c837ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "label_encoded_data = data.copy()\n",
    "label_encoders = {}"
   ],
   "id": "8cc316de039f21f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code sets up the environment for label encoding.\n",
    "\n",
    "Line1: Copying the dataset\n",
    "\n",
    "Line2: Initializing the Encoders Dictionary"
   ],
   "id": "3f75c88b965669d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run lable-encoder for every previosly defined columne (function imported from sklearn)",
   "id": "1515da396df78c06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    label_encoded_data[col] = le.fit_transform(label_encoded_data[col])\n",
    "    label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(\"\\nLabel-Encoded Data for Visualisation:\")\n",
    "print(label_encoded_data.head())"
   ],
   "id": "92396d934e28d479"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This loop iterates over each categorical column and applies Label Encoding transforming string labels into numeric codes.\n",
    "\n",
    "1. for col in categorical_columns: Loops through each column listed earlier\n",
    "2. le = labelEncoder(): Creates a new LabelEncoder instance from scikit-learn for the current column\n",
    "3. label_encoded_data[col] = le.fit_transform(label_encoded_data[col]): Fits the encoder to the column's categories and transforms them into integers + Replaces the original text values in label_encoded_data with the corresponding numeric labels\n",
    "4. label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_))): Stores the mapping of original category names to their encoded values in the label_encoders dictionary + This allows to trace or reverse the encoding later if needed"
   ],
   "id": "9fecd8ff6cc49431"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### concatinates x and y into one point to be visualized",
   "id": "dd4fb006cbf3b865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = pd.concat([label_encoded_data], axis=1)",
   "id": "f375ebafb2613c84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new DataFrame called 'all_data_LableEncoded' by concatenating 'label_encoded_data' along the column axis 'axis=1'",
   "id": "a7984b3a89fbfec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Drops all columes that are non-numeric to make scaling possible",
   "id": "d39e31beaca0636e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = all_data_LableEncoded.select_dtypes(include=['number'])",
   "id": "9c96f2c72f3a5720"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line filters the dataset to keep only the numeric columns from 'all_data_LableEncoded'.",
   "id": "eb9ddeb3e01426c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scale data (normalized via MinMaxScaler - between 0 and 1)\n",
    "sscaler = preprocessing.StandardScaler()    ???\n",
    "\n",
    "all_data_LableEncoded = sscaler.fit_transform(all_data_LableEncoded)    ???"
   ],
   "id": "9f12bd3a7f800f5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nscaler = preprocessing.MinMaxScaler()\n",
    "all_data_LableEncoded = nscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "1a75a73ea660129a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs Min-Max Scaling on the numeric features in the dataset, transforming them into a commonscale between 0 and 1.\n",
    "1. Line1: Initalizes a MinMaxScaler object from scikit-learn\n",
    "2. Line2: Calculates the min and max values for each feature iin the dataset and applies the scaling transformation to each value"
   ],
   "id": "eae25a6e0d894e17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisation before cleaning Data",
   "id": "4c2cc9b27675b657"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reintegrates Column name for boxplot",
   "id": "7e08f375a59ed8a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scaled_df = pd.DataFrame(all_data_LableEncoded, columns=label_encoded_data.select_dtypes(include='number').columns)",
   "id": "5aff4042b4b9c951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line converts the scaled NumPy array (from the Min-Max Scaler) back into a pandas DataFrame and restores the original column names.",
   "id": "10bb9b3a7289d56f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Boxplot with readable x-axis",
   "id": "612a72bd7c435c8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=scaled_df, orient='v', palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Normed boxplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "87c04c0b96869e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot for each feature in the 'scaled_df' DataFrame to visualize the distribution and spread of the normalized (Min-max scaled) data.\n",
    "1. Line1: Sets the size of the figure to be 12 inches wide by 6 inches tall and ensures the plot is large enough to accommodate all features without crowding.\n",
    "2. Line2: Creates a vertical boxplot for each column in the 'scaled_df' DataFrame and uses Seaborn's elegant and color-friendly 'Set2' palette. Each box shows the median, the IQR and Whiskers & Outliers.\n",
    "3. Line3: Rotates the x-axis labels by 45 degrees for better readability, especially when there are many features.\n",
    "4. Line4: Adds a title to the plot for context, signaling that the data is normalized.\n",
    "5. Line5: Adjusts spacing to prevent overlap between axis labels, titles, and plot content.\n",
    "6. Line6: Renders and displays the final plot in the notebook."
   ],
   "id": "ead6b42ead1e6a49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Boxplot for only numerical data",
   "id": "e0974975b2686d02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_cols = ['selling_price', 'km_driven', 'year']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=scaled_df[selected_cols], orient='v', palette='Set3')\n",
    "plt.title(\"Normed boxplot for numerical data only\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a91e149b8d9268ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This blocks generates a boxplot visualization focused on three specific, scaled numerical features:\n",
    "\n",
    "'selling_price'\n",
    "\n",
    "'km_driven'\n",
    "\n",
    "'year'\n",
    "\n",
    "1. Line1: Selects the subset of important numerical features for focused analysis.\n",
    "2. Line2: Sets the plot size to be 8 inches wide and 5 inches tall.\n",
    "3. Line3: Creates a vertical boxplot for just the selected columns using the soft, pastel 'Set3' color palette from Seaborn.\n",
    "4. Line4: Adds a descriptive title to clarify that this plot shows normalized (scaled) numerical features.\n",
    "5. Line5: Ensures layout is adjusted for neatness and then displays the plot."
   ],
   "id": "fb891a4313a2a18b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pairplot to show correlation",
   "id": "528da982deb168c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.pairplot(scaled_df[selected_cols])\n",
    "plt.suptitle(\"Pairplot for select charactaristics\", y=1.02)\n",
    "plt.show()"
   ],
   "id": "3cfc536eb1a2bc9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This blocks creates a pairplot to visually explore pairwise relationships among the selected numerical features:\n",
    "\n",
    "'selling_price'\n",
    "\n",
    "'km_driven'\n",
    "\n",
    "'year'\n",
    "\n",
    "1. Line1: Creates a grid of scatterplots for each pairwise combination of the selected features. This helps to visualize Correlations, Clustering tendencies and Linearity or Non-Linearity Relationships. The Histograms are shown on the diagonal to represent each variable's distribution.\n",
    "2. Line2: Adds a super title above the entire plot grid.\n",
    "3. Line3: Renders the entire pairplot for viewing."
   ],
   "id": "1f5e8aa1560acaa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean Data & Create variable Brand",
   "id": "65f778ecab77450"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove missing data",
   "id": "4c8f6e7a09542f39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = data.dropna()",
   "id": "2478576eb668d23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line removes all rows with missing values from the 'data' DataFrame.",
   "id": "6cb30eeafdc41003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Show how much data was removed",
   "id": "9361739167ddf637"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Data after removing data:\")\n",
    "print(data.isnull().sum())\n",
    "print(f\"Remaining rows: {len(data)}\")"
   ],
   "id": "4723ea92f78c504c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block checks and confirms that all missing values have been removed from the dataset and reports the number of remaining rows.\n",
    "\n",
    "1. Line1: Prints a header to indicate that the following output relates to the cleaned dataset.\n",
    "2. Line2: Checks for missing values in each column of the 'data' DataFrame, creates a Boolean mask of the same shape as the data and then counts the number of 'True' values in each column, i.e. the number of missing entries.\n",
    "3. Line3: Prints the total number of rows left in the dataset after dropping rows with missing values using 'len(data)'."
   ],
   "id": "7320d64febbb147b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### IQR-based Removal of Outliers",
   "id": "5e13c2d862a9c841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = np.percentile(df[column], 25)\n",
    "    Q3 = np.percentile(df[column], 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]"
   ],
   "id": "6c2d64ddee25c33f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function removes outliers from a specific column in a DataFrame using the Interquartile Range (IQR) method\n",
    "\n",
    "1. 'Q1': 25th percentile - the value below which 25% of the data falls\n",
    "2. 'Q3': 75th percentile - the value below which 75% of the data falls\n",
    "3. 'IQR = Q3-Q1': IQR is the spread of the middle 50% of values and it is used to understand the natural range of variation in the data.\n",
    "4. Line5 + Line6: These define the acceptable range and any values below the lower bound or above the upper bound are considered outliers.\n",
    "5. Line7: Returns a filtered version of the original DataFrame, keeping only the rows where the specified column's value is within the acceptable range -> Outlier are removed."
   ],
   "id": "808fb431c2c8ca37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Apply to most important column",
   "id": "13bfcaa9e510db9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = remove_outliers_iqr(data, 'selling_price')\n",
    "data = remove_outliers_iqr(data, 'km_driven')"
   ],
   "id": "bc179ef93d798956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These lines apply the IQR-based outlier removal function to two important columns in the dataset: 'selling_price' and 'km_driven'.\n",
    "\n",
    "1. Line1: Removes rows where 'selling_price' is considered an outlier based on the IQR rule and keeps only cars with selling prices within the typical range.\n",
    "2. Line2: Applies the same IQR filtering to the 'km_driven' column and eliminates unusually low or high mileage entries that could distort statistical analysis or model training."
   ],
   "id": "8d99f03f1ca00747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nDaten nach IQR-basierter Ausrei√üerbereinigung:\")\n",
    "print(f\"Max. Verkaufspreis: {data['selling_price'].max()}\")\n",
    "print(f\"Max. Kilometerstand: {data['km_driven'].max()}\")\n",
    "print(f\"Verbleibende Zeilen nach IQR-Filter: {len(data)}\")"
   ],
   "id": "e8b00c4db8c02c69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block prints a quick summary of the dataset after removing outliers using the IQR method.\n",
    "1. Line1: Prints a headline used for clarity when reading the console output.\n",
    "2. Line2: Displays the maximum selling price in the cleaned dataset which helps verifying that extremely high prices have been removed.\n",
    "3. Line3: Shows the maximum odometer reading after outlier removal and ensures that unusally high mileage values have been filtered out.\n",
    "4. Line4: Prints the number of remaining rows in the dataset which tells how much data is left after removing rows that contained outliers in 'selling_price' and 'km_driven'."
   ],
   "id": "bab3eeed17c51636"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create 'Brand' as new column",
   "id": "7c3bf4ed9e5eac31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data['brand'] = data['name'].str.split().str[0]\n",
    "print(data)"
   ],
   "id": "1a35fe3e1026f49e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new column called 'brand' by extracting the first word from the 'name' column which represents the car brand.",
   "id": "1edce02663cc2c28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label-Encoding for Visualisation after cleaning",
   "id": "d7a160b07af3d056"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ???",
   "id": "9d58975c2852af76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\n",
    "label_encoded_data = data.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    label_encoded_data[col] = le.fit_transform(label_encoded_data[col])\n",
    "    label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(\"\\nLabel-Encoded Data (nur zur Referenz):\")\n",
    "print(label_encoded_data.head())"
   ],
   "id": "2f3670cbf0cfa6df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs label encoding on selected categorical columns, converting them from text values to integers so they can be used in regression models.\n",
    "\n",
    "1. Line1: Specifies the list of categorical features to be encoded which are typically textual descriptors that must be converted into numeric format for modeling.\n",
    "2. Line2: Creates a copy of the original dataset to apply the encodings without altering the raw data.\n",
    "3. Line3: Initializes an empty dictionary to store the encoding mappings for each categorical column.\n",
    "4. The 'for' loop: Iterates over each column in the 'categorical_columns' list; The 'LabelEncoder()' from scikit-learn is used to convert category labels into integers. The transformed values replace the original column in 'label_encoded_data'. The mapping of original class labels to integer codes is stored in 'label_encoders' for reference or inverse transformation later.\n",
    "5. Line10 + Line11: Displays the first few rows of the updated dataset to confirm that the categorical features have been encoded."
   ],
   "id": "fad31c9e1922f96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### concatinate x and y into one point to be visualized",
   "id": "496f74c3b81afbcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = pd.concat([label_encoded_data], axis=1)",
   "id": "90feb61d4cc86b9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new DataFrame called 'all_data_LableEncoded' by concatenating the contents of 'label_encoded_data' along the columns axis.",
   "id": "7f631765aeb1c740"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Drop all columns that are non-numeric to make scaling possible",
   "id": "5a033f688a670b2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = all_data_LableEncoded.select_dtypes(include=['number'])",
   "id": "534cde6690518511"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line filters the 'all_data_LableEncoded' DataFrame to include only numeric columns, removing any that are not numeric.",
   "id": "9a3f1545a3b99ad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scale data (normalized)\n",
    "\n",
    "sscaler = preprocessing.StandardScaler()\n",
    "\n",
    "all_data_LableEncoded = sscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "4b07c06cb19fef75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nscaler = preprocessing.MinMaxScaler()\n",
    "all_data_LableEncoded = nscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "ec105be81b324dc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code applies Min-Max Scaling to normalize all numeric features in the dataset, transforming their values to a common range between 0 and 1.\n",
    "1. Line1: Initializes a MinMaxScaler object from scikit-learn's 'preprocessing' module and will scale each feature individually.\n",
    "2. Line2: Calculates the minimum and maximum values for each feature and scales each value in the dataset to the 0-1 range."
   ],
   "id": "2d24f8339d565fba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization after Data cleaning",
   "id": "26e221dbf855a14d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scale Data ???",
   "id": "db27b50893bcf74b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scaled_df = pd.DataFrame(all_data_LableEncoded, columns=label_encoded_data.select_dtypes(include='number').columns)",
   "id": "45db41f2efeda0d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line converts the scaled NumPy array back into a pandas DataFrame and restores the original column names, making the data human-readable and easier to work with.",
   "id": "1ba9c5d0affc2cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Boxplot with readable axis names",
   "id": "46e216caa2154a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=scaled_df, orient='v', palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot der skalierten numerischen Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ba4be1f0e93359fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot for all features in the 'scaled_df' DataFrame, which contains only scaled numeric data. The plot helps visually inspecting the distribution and variability of each feature.\n",
    "1. Line1: Sets the figure size to 12 inches wide by 6 inches tall for better readability.\n",
    "2. Line2: Uses Seaborn to draw vertical boxplots for each numeric feature in 'scaled_df' and 'palette=Set2' gives the plot a soft, color-coded appearance to distinguish features visually.\n",
    "3. Line3: Rotates the x-axis labels by 45 degrees so that long feature names dont overlap and remain legible.\n",
    "4. Line4: Adds a descriptive title\n",
    "5. Line5 + Line6: Adjusts the layout to avoid overlapping elements and displays the final plot."
   ],
   "id": "8a94c98254259a59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Boxplot only for selected numeric columns",
   "id": "e372e0ea45e7af3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_cols = ['selling_price', 'km_driven', 'year']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=scaled_df[selected_cols], orient='v', palette='Set3')\n",
    "plt.title(\"Boxplot ausgew√§hlter Merkmale\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f572d28726daa6b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot visualization for a selected subset of key numeric features: 'selling_price', 'km_driven' and 'year', all of which have been previously scaled to 0-1 range.\n",
    "1. Line1: Selects the three features for targeted visualization.\n",
    "2. Line2: Sets the figure size to 8 inches wide and 5 inches tall for compact clarity.\n",
    "3. Line3: Draws vertical boxplots for just the selected columns using Seaborn's pastel 'Set3' color palette.\n",
    "4. Line4: Adds a title.\n",
    "5. Line5 + Line6: Adjusts spacing to prevent overlap and displays the plot."
   ],
   "id": "d2798ae873fb6d6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Pairplot for Distribution and Correlation",
   "id": "ae3ca5a17e95eb62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.pairplot(scaled_df[selected_cols])\n",
    "plt.suptitle(\"Paarweise Verteilungen ausgew√§hlter Merkmale\", y=1.02)\n",
    "plt.show()\n",
    "#print(\"Trainingsdaten zus√§tzlich als 'prepared_used_car_data_train.parquet' gespeichert.\")\n",
    "#print(\"Testdaten zus√§tzlich als 'prepared_used_car_data_test.parquet' gespeichert.\")\n",
    "#print(\"Gesamtdaten zus√§tzlich als 'prepared_used_car_data.parquet' gespeichert.\")"
   ],
   "id": "c7d5546675dd85e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a pairplot that visualizes the pairwise relationships and distributions of three selected, scaled features: 'selling_price', 'km_driven' and 'year'.\n",
    "1. Line1: Generates a grid of plots: Scatter plots and Histograms.\n",
    "2. Line2: Adds a descriptive title above the plot grid; 'y=1.02' adjusts the title position slightly above the plot area to prevent overlap.\n",
    "3. Line3: Renders and displays the plot."
   ],
   "id": "42e4cd0eb232d81c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### One-Hot-Encoding for Regression Model Training & Testing",
   "id": "34beaaa83dfb659d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### One-Hot-Encoding for categorical Variables",
   "id": "424611b424910f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ncoded_data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(\"\\nOne-Hot-Encoded Data:\")\n",
    "print(encoded_data.head())\n",
    "\n",
    "print(encoded_data)"
   ],
   "id": "a3632ce25d57c669"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block uses one-hot encoding to transform categorical columns in the dataset into binary 0 or 1 columns, making them suitable for regression models.\n",
    "1. Line1: Performs one-hot encoding on the columns listed in 'categorical_columns'. For each unique category in these columns, new binary columns are created. Drops the first category for each column to avoid multicollinearity when using models like linear regression.\n",
    "2. Line3: Prints a header for clarity in console output.\n",
    "3. Line4: Displays the first few rows of the encoded dataset for a quick preview.\n",
    "4. Line6: Prints the entire DataFrame, which now contains both numeric and one-hot encoded binary columns."
   ],
   "id": "61a0d41d69124fa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sort data by 'year' and 'km_driven'",
   "id": "86c04d61d9b69e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "encoded_data_sorted = encoded_data.sort_values(by=['year', 'km_driven'], ascending=[False, True])",
   "id": "a29562d18d8eab85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line sorts the encoded dataset based on two columns - 'year' and 'km_driven' - to organize the data in a meaningful order.",
   "id": "b5b2c20ee3fda993"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare data used for regression analysis",
   "id": "f5c866911a853cd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features = encoded_data_sorted.columns.drop(['name', 'selling_price'])\n",
    "target = 'selling_price'\n",
    "\n",
    "X = encoded_data_sorted[features]\n",
    "y = encoded_data_sorted[target]"
   ],
   "id": "7967234d07de09d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block prepares the feature matrix 'X' and target vector 'Y' for training a regression model, using sorted and one-hot encoded dataset.\n",
    "1. Line1: Selects all column names except: 'name' and 'selling_price' -> Result is a list of input features for the model.\n",
    "2. Line2: Explicitly defines 'selling_price' as the target variable.\n",
    "3. Line4: Creates the feature matrix 'X' by selecting only the columns in 'features' from the dataset; 'X' will be used as input for the regression model\n",
    "4. Line5: Creates the target vector 'Y', which contains the selling prices (values to be predicted)"
   ],
   "id": "a8b181cbc9a123a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving test data",
   "id": "a4bae775de587331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_data = pd.concat([X, y], axis=1)\n",
    "all_data.to_csv('prepared_used_car_data_all.csv', index=False)"
   ],
   "id": "1487581c01f6a249"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block recombines the feature 'X' and target 'Y' into a single DataFrame and saves it as a '.csv' file for future use.\n",
    "1. Line1: Concatenates the feature matrix 'X' and the target vector 'Y' horizontally and reconstructs the full dataset 'all_data' with both inputs and outputs in one table.\n",
    "2. Line2: Saves the combined dataset to a CSV file and ensures that the DataFrame index is not written to the file, keeping the output clean and suitable for reuse."
   ],
   "id": "ce5595b313fc27d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creation of training and test data",
   "id": "be0ae433862329da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "id": "532c67feaf5f924f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line uses scikit-learn's 'train_test_split()' function to divide the dataset into training and testing subsets, a crucial step for evaluating regression models.",
   "id": "1d5e917ccd7fedec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving of training data",
   "id": "d658474144423a97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data.to_csv('prepared_used_car_data_train.csv', index=False)"
   ],
   "id": "a1134ca92ea2ee04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block combines the training features and labels into a single DataFrame and then exports it to a '.csv' file for storage or reuse.\n",
    "1. Line1: Merges the training input features 'X_train' and then training target values 'Y_train' side by side (along columns) and produces a single DataFrame 'train_data' that contains all the necessary data for training a model.\n",
    "2. Line2: Saves the 'train_data' DataFrame as a CSV file and ensures the row indices are not written into the file, keeping it clean and easy to reload."
   ],
   "id": "b5b767072458240"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving of test data",
   "id": "8ae9c1209c27c6b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data.to_csv('prepared_used_car_data_test.csv', index=False)"
   ],
   "id": "f5ecb1a8e5800d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code combines the test features and labels into a single DataFrame and then saves it as a CSV file for future use or evaluation.\n",
    "1. Line1: Merges the test feature set 'X_test' and the corresponding target values 'Y_test' horizontally and produces a new DataFrame 'test_data' that includes all the columns needed to evaluate a regression model.\n",
    "2. Line2: Saves the resulting test dataset to a CSV file and prevents the row index from being included in the file, making the CSV clean and readable."
   ],
   "id": "b417829164c3d2d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confirming saved files",
   "id": "b53fcc1b31f27a5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nTrainingsdaten gespeichert als 'prepared_used_car_data_train.csv'\")\n",
    "print(\"Testdaten gespeichert als 'prepared_used_car_data_test.csv'\")"
   ],
   "id": "144ab05b37ea8757"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These print statements simply confirm to the user that the training and test datasets have been successfully saved to CSV files.\n",
    "1. Line1: Outputs a message confirming that the training data was saved.\n",
    "2. Line2: Confirms that the test data was also saved."
   ],
   "id": "e728efe85e240e7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KNN",
   "id": "913fdd0c6b08df1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Set Up",
   "id": "ef36d85c8da1450c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import DataPrep\n",
    "\n",
    "(X_train_KNN, X_test_KNN, Y_train_KNN, Y_test_KNN) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)"
   ],
   "id": "511b1e9da195258b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block sets upo the environment to train and evaluate a K-Nearest Neighbors Regression model using a train/test split of preprocessed data.\n",
    "\n",
    "1. Line1: Imports the KNN regressor from scikit-learn\n",
    "2. Line2: Adds a tool for hyperparameter tuning by trying different values and splits the dataset into training and testing sets\n",
    "3. Line3: Imports the DataPrep file\n",
    "4. Line5: Splits features and target into a training set and testing set while ensuring that the split is reproducible"
   ],
   "id": "88890b73da1c3e46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Initialize KNN model",
   "id": "96f94c4c912cc921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "knnmodelCV = KNeighborsRegressor()",
   "id": "2294d053ab7fc4a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates an instance of the KNN model from scikit-learn with default parameters.",
   "id": "87886d8e6cbaffdc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define grid of neighbor counts",
   "id": "7565349b274051ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "'n_neighbors': range(3, 22, 2),\n",
    "'weights': ['uniform', 'distance'],\n",
    "'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "'leaf_size': [8, 16, 32, 64, 128, 256, 512],\n",
    "'p': [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "}"
   ],
   "id": "1ac6f8cb0f3ca853"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block defines a hyperparameter grid that will be used to tune a KNN regressor using tools like GridSearchCV.\n",
    "\n",
    "1. Line2: Tries odd values for k from 3 to 21 which defines how many neighbors the model considers when making predictions.\n",
    "2. Line3: 'uniform' makes sure all neighbors contribute equally to the prediction and 'distance' makes sure that closer neighbors contribute more to the prediction than distant ones.\n",
    "3. Line4: Specifies the search algorithm used to find neighbors. 'auto' -> Chooses the best algorithm based on the data. 'ball_tree' and 'kd_tree' -> Use tree structure for fast searches. 'brute' -> Calculates distances directly.\n",
    "4. Line5: Affects the speed vs. memory tradeoff in tree based algorithms. Smaller values result in faster query time. Larger values result in faster tree building.\n",
    "5. Line6: Specifies the power parameter for the Minkowski distance."
   ],
   "id": "9fa17ca71204c291"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run 10-fold cross-validation across neighbor settings",
   "id": "6dcc98717ded8420"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_knnmodel = GridSearchCV(estimator=knnmodelCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_knnmodel.fit(X_train_KNN, Y_train_KNN)"
   ],
   "id": "982ed5930d7a7c0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code uses GridSearchCV from scikit-learn to perform an exhaustive search over a defined set of hyperparameters for a KNN regression model.\n",
    "Assigns all available CPU cores to parallelize the search and speed up computation and uses 10-fold cross-validation."
   ],
   "id": "8e884a5c287dd7b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output the best number of neighbors",
   "id": "68d7a16cecfb1ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Best parameters found:\", CV_knnmodel.best_params_)",
   "id": "ab08d576e3bc342c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line prints the optimal set of hyperparameters that were found during the GridSearchCV process.",
   "id": "23271a575ac5ca71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating Model Performance",
   "id": "77ba83f7b6489e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_knnmodel.predict(X_train_KNN)\n",
    "Y_train_dev = sum((Y_train_KNN - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_KNN - Y_train_KNN.mean())**2)\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev"
   ],
   "id": "474d1ff7e08dbfa5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually calculates the coefficient of determination (r2) to evaluate how well the KNN model fits the training data.\n",
    "1. Line1: Uses the trained cross-validated KNN model to predict selling prices for the training set.\n",
    "2. Line2: Calculates the residual sum of squares (RSS) and measures how far off the predictions are from the actual values which should be minimal.\n",
    "3. Line3: Calculates the total sum of squares (TSS) and measures the total variation in the target variable (how far are values from the mean)\n",
    "4. Line4: Calculates the coefficient of determination (r2)."
   ],
   "id": "3cad2e436bf95347"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict on test set",
   "id": "2bfe85c50a2cfb01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_knnmodel.predict(X_test_KNN)\n",
    "Y_test_dev = sum((Y_test_KNN - Y_test_pred)**2)\n",
    "Y_test_meandev = sum((Y_test_KNN - Y_test_KNN.mean())**2)\n",
    "pseudor2 = 1 - Y_test_dev / Y_test_meandev\n",
    "\n",
    "DataPrep.report.loc[len(DataPrep.report)] = [\"KNN_LE \", r2, pseudor2,\"\", CV_knnmodel.cv_results_['mean_test_score'][CV_knnmodel.best_index_], CV_knnmodel.cv_results_['std_test_score'][CV_knnmodel.best_index_]]"
   ],
   "id": "63c9f4fdd36f8ca2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block evaluates the KNN model on the test set using the coefficient of determination and calculates a pseudo r2 score for generalization performance and appends a summary row to report DataFrame for later comparison with other models.\n",
    "1. Line1: Uses the best tuned KNN model to predict the selling prices for the test set.\n",
    "2. Line2: Calculates the residual sum of squares (RSS) on the test data.\n",
    "3. Line3: Computes the total sum of squares (TSS) of the actual test values\n",
    "4. Line4: Calculates pseudo-r2 which is the models explanatory power on the unseen test data.\n",
    "5. Line6: Logs results to a report."
   ],
   "id": "a9353e03545b53f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Encoded Data for Training and Testing",
   "id": "1c32ccb1e13d0a21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train_KNN, X_test_KNN, Y_train_KNN, Y_test_KNN) = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "83765b9d7b1f833b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the one-hot encoded dataset into training and testing sets for building and evaluating a KNN model.",
   "id": "583ba6eca30b9ed3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run 10-fold cross-validation across neighbor settings",
   "id": "ca064ee8b924495d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_knnmodel = GridSearchCV(estimator=knnmodelCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_knnmodel.fit(X_train_KNN, Y_train_KNN)"
   ],
   "id": "aeb112676327e6be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs hyperparameter optimization for the KNN regression model using one-hot encoded data with cross validation to ensure reliable model selection.\n",
    "1. Line1: Performs an exhaustive grid search over the parameter combinations defined in 'param_grid'.\n",
    "2. Line2: Trains and evaluates all hyperparameter combinations using the training data based on one-hot encoded features."
   ],
   "id": "582037eb7717b901"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output the best number of neighbors",
   "id": "df30201764aec89c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Best parameters found:\", CV_knnmodel.best_params_)\n",
    "Y_train_pred = CV_knnmodel.predict(X_train_KNN)\n",
    "Y_train_dev = sum((Y_train_KNN - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_KNN - Y_train_KNN.mean())**2)\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev"
   ],
   "id": "6e43f95c135b9312"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This line prints the optimal hyperparameter combination selected by 'GridSearchCV' for the KNN regression model trained on one-hot encoded data.\n",
    "1. Line2: Predicts the selling prices for the training data using the best-found KNN model.\n",
    "2. Line3: Computes the residual sum of squares (RSS) and measures how much error remains after using the models predictions.\n",
    "3. Line4: Computes the total sum of squares (TSS) and represents the total variance in the training labels serving as a baseline.\n",
    "4. Line5: Calculates the r2 score which explains how much variation is explained by the model."
   ],
   "id": "831cd79491d672da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict on test set",
   "id": "3432bff5642ee100"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_knnmodel.predict(X_test_KNN)\n",
    "Y_test_dev = sum((Y_test_KNN - Y_test_pred)**2)\n",
    "Y_test_meandev = sum((Y_test_KNN - Y_test_KNN.mean())**2)\n",
    "pseudor2 = 1 - Y_test_dev / Y_test_meandev"
   ],
   "id": "eb5880e8f8375013"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block evaluates how well the tuned KNN model generalizes to unseen data by manually calculating the pseudo r2 score.\n",
    "1. Line1: Uses the trained KNN model to predict selling prices for the test set.\n",
    "2. Line2: Computes the residual sum of squares (RSS) for the test set and measures the models prediction error on unseen data.\n",
    "3. Line3: Compues the total sum of squares (TSS) for the test set and represents the total variance in the actual test values serving as a baseline.\n",
    "4. Line4: Calculates the pseudo-r2 and indicates how well the model generalizes to new unseen data."
   ],
   "id": "1b6128865347c811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Log KNN Model results in a report",
   "id": "db8277422d3a75cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DataPrep.report.loc[len(DataPrep.report)] = [\"KNN_OH \", r2, pseudor2,\"\", CV_knnmodel.cv_results_['mean_test_score'][CV_knnmodel.best_index_], CV_knnmodel.cv_results_['std_test_score'][CV_knnmodel.best_index_]]\n",
    "print(DataPrep.report.head())\n",
    "print(CV_knnmodel.best_params_)"
   ],
   "id": "3661288c5440411b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block logs the performance metrics of the KNN model trained with one-hot encoded data into a centralized evaluation report and then displays the report and the best hyperparameters.",
   "id": "fd8549bb992fcc71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neural Networks",
   "id": "c0298822913ab143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preparation to train a Neural Network",
   "id": "2459a43857effd0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import DataPrep"
   ],
   "id": "d1a96d3bda5b49bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block sets up the environment for training and tuning a neural network regression model using scikit-learn's 'MLPRegressor' with data and configuration coming from a shared module called 'DataPrep'.\n",
    "1. Line1: Used to split data into training and testing sets and enables hyperparameter tuning using exhaustive search with cross-validation.\n",
    "2. Line2: Imports 'MLPRegressor' a multilayer perceptron for regression tasks.\n",
    "3. Line3: Imports the 'DataPrep' module."
   ],
   "id": "ea18a40661d12073"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting Label-Encoded Data",
   "id": "83ff98b11f54bde9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_nn, X_test_nn, Y_train_nn, Y_test_nn) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)",
   "id": "ed26efe58f08b64e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the label-encoded dataset into training and testing sets preparing it for use in training a neural network regression model.",
   "id": "938be294c034fd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining a Hyperparameter Grid for Tuning the Network Regressor",
   "id": "8cfbc2704d7e93e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "'hidden_layer_sizes': [(5,), (8,), (10,), (13,)],\n",
    "'alpha': [0.0, 0.0025, 0.005, 0.0075, 0.01, 0.1],\n",
    "'activation': ['logistic', 'tanh', 'relu'],\n",
    "'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "'max_iter': [5000],\n",
    "'random_state': [0],\n",
    "'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "}"
   ],
   "id": "11c13350df76b1fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block defines a parameter grid to be used with 'GridSearchCV' for tuning a 'MLPRegressor' model in a regression task. Each key represents a hyperparameter and each value is a list of options that 'GridSearchCV' will explore during cross-validation.\n",
    "1. Line2: Defines the structure of the hidden layers; how many neurons\n",
    "2. Line3: L2 regularization term which helps to prevent overfitting by penalizing large weights\n",
    "3. Line4: Specifies the activation function used in the hidden layers\n",
    "4. Line5: Optimization algorithm used for training\n",
    "5. Line6: Sets the maximum number of training iterations to 5000\n",
    "6. Line7: Ensures reproducibility by fixing the random seed used inernally by the model\n",
    "6. Line8: Controls how the learning rate adapts over time when using 'sgd' or 'adam'"
   ],
   "id": "c2a8259638519a2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training a Neural Network with Grid Search and Cross-Validation",
   "id": "3b317525d6e67e84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NNetRregCV = MLPRegressor()\n",
    "CV_nnmodel = GridSearchCV(estimator=NNetRregCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_nnmodel.fit(X_train_nn, Y_train_nn)"
   ],
   "id": "e0102299ad7865d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates, tunes and trains a neural network regressor using scikit-learn's 'MLPRegressor' and 'GridSearchCV'.\n",
    "1. Line1: Initializes a Multi-Layer Perceptron (MLP) Regressor which is a type of feedforward neural network.\n",
    "2. Line2: Wraps the neural network model with 'GridSearchCV' to perform automated hyperparameter tuning.\n",
    "3. Line3: Trains the model using the training data and runs the grid search."
   ],
   "id": "ed941239c09b3d13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the Neural Network on Training Data",
   "id": "15928d9333a02d79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_nnmodel.predict(X_train_nn)\n",
    "Y_train_dev = sum((Y_train_nn - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_nn - Y_train_nn.mean())**2)  # [aus PDF]\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev  # [aus PDF]"
   ],
   "id": "6c478d8829a7dc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually calculates the r2 score to evaluate how well the trained neural network model fits the training data.\n",
    "1. Line1: Uses the best neural network model selected by 'GridSearchCV' to predict target values for the training set\n",
    "2. Line2: Calculates the residual sum of squares (RSS) and measures the total prediction error of the model\n",
    "3. Line3: Calculates the total sum of squares (TSS) and measures how much variance is in the target data without any model\n",
    "4. Line4: Computes the r2 score which explains the variance in the training data"
   ],
   "id": "6223243634634b44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the Neural Network on Test Data",
   "id": "2d660e23601c4437"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_nnmodel.predict(X_test_nn)\n",
    "Y_test_dev = sum((Y_test_nn - Y_test_pred)**2)\n",
    "Y_train_meandev = sum((Y_test_nn - Y_test_nn.mean())**2)  # [aus PDF]\n",
    "pseudor2 = 1 - Y_test_dev / Y_train_meandev"
   ],
   "id": "aebea710b755b8c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block calculates the pseudo r2 score to assess how well the trained neural network regressor performs on unseen test data.\n",
    "1. Line1: Predicts target values for the test set using the neural network model selected by 'GridSearchCV'\n",
    "2. Line2: Computes the residual sum of squares (RSS) for the test set which represents the total prediction error of the model on unseen data\n",
    "3. Line3: Computes the total sum of squares (TSS) for the test set which represents the total variance in the target values\n",
    "4. Line4: Calculates the pseudo r2 score used to evaluate the model performance on the test data by measuring how much variability in the test target data is explained by the model"
   ],
   "id": "80b89b249790996e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logging Neural Network Results into the Model Comparison Report",
   "id": "c81f7d6571abd4c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DataPrep.report.loc[len(DataPrep.report)] = [\"NN_LE \", r2, pseudor2,\"\", CV_nnmodel.cv_results_['mean_test_score'][CV_nnmodel.best_index_], CV_nnmodel.cv_results_['std_test_score'][CV_nnmodel.best_index_]]",
   "id": "8b4cf16056238160"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line adds a new row to the shared 'DataPrep.report' table documenting the performance metrics of the neural network model trained with label-encoded data.",
   "id": "c9f229704b383866"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Displaying the best hyperparameters",
   "id": "72a680a2d62ca81d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(CV_nnmodel.best_params_)",
   "id": "97f05494fbc64807"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line prints the optimal hyperparameter configuration found by 'GridSearchCV' during training of the MLPRegressor",
   "id": "64cda8f9c77e2f5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Encoded Data for Neural Network Training and Testing",
   "id": "c6b8a7dfa53e9c80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_nn, X_test_nn, Y_train_nn, Y_test_nn) = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "8598d422ae567bcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the dataset - specifically the one-hot encoded version - into a training set and a test set preparing it for use with a neural network regression model",
   "id": "511f6ad493efcda3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Grid Search to Train and Tune a Neural Network with One-Hot Encoded Data",
   "id": "80956590c17d9861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_nnmodel = GridSearchCV(estimator=NNetRregCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_nnmodel.fit(X_train_nn, Y_train_nn)"
   ],
   "id": "3dbc889752ac0a25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code performs automated hyperparameter tuning using 'GridSearchCV' for a neural network regression model trained on one-hot encoded features.\n",
    "1. Line1: Wraps the 'MLPRegressor' in a 'GridSearchCV' object to perform an exhaustive search over a grid of hyperparameters\n",
    "2. Line2: Trains the model using the training data"
   ],
   "id": "3acce970a616e7b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculating r2 score for Neural Network",
   "id": "57fd82fe1a0c50ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_nnmodel.predict(X_train_nn)\n",
    "Y_train_dev = sum((Y_train_nn - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_nn - Y_train_nn.mean())**2)  # [aus PDF]\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev  # [aus PDF]"
   ],
   "id": "3fd612181e6e9775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually computes the r2 score for the neural network model trained on one-hot encoded features using the training dataset\n",
    "1. Line1: Predicts the target values on the training data using the best neural network selected by 'GridSearchCV'\n",
    "2. Line2: Calculates the residual sum of squares (RSS) which represents the total prediction error of the model on the training set.\n",
    "3. Line3: Calculates the total sum of squares (TSS) which represents the total variance in the training data assuming a baseline model that always predicts the mean\n",
    "4. Line4: Computes the r2 score which measures how well the model explains the variance in the target variable"
   ],
   "id": "5d117c4552ead93e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculating pseudo r2 score for Neural Network",
   "id": "69e55a4b832b76bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_nnmodel.predict(X_test_nn)\n",
    "Y_test_dev = sum((Y_test_nn - Y_test_pred)**2)\n",
    "Y_train_meandev = sum((Y_test_nn - Y_test_nn.mean())**2)  # [aus PDF]\n",
    "pseudor2 = 1 - Y_test_dev / Y_train_meandev"
   ],
   "id": "af847fb3088b532d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code block calculates the r2 score on the test dataset (pseudo r2) to evaluate the generalization performance of the neural network trained on one-hot encoded features.\n",
    "1. Line1: Uses the trained neural network to predict values for the test set.\n",
    "2. Line2: Computes the residual sum of squares (RSS) on the test set and measures the total prediction error made by the model on unseen data\n",
    "3. Line3: Calculates the total sum of squares (TSS) on the test data\n",
    "4. Line4: Calculates the test r2 (pseudo r2) which reflects how well the model generalizes to new data"
   ],
   "id": "316893efb4ea9d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logging Neural Network Results to the Report",
   "id": "27a702d732430c9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DataPrep.report.loc[len(DataPrep.report)] = [\"NN_OH \", r2, pseudor2,\"\", CV_nnmodel.cv_results_['mean_test_score'][CV_nnmodel.best_index_], CV_nnmodel.cv_results_['std_test_score'][CV_nnmodel.best_index_]]\n",
    "print(DataPrep.report.head())\n",
    "print(CV_nnmodel.best_params_)"
   ],
   "id": "b402a7e2b5bed92d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block adds a new row to your 'DataPrep.report' table to document the performance of a neural network model trained on one-hot encoded features and then displays the first few rows of the report and the best hyperparameters found during grid search.",
   "id": "d3f90190be154f4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OLS Regression",
   "id": "8b88c6098b6e4ade"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Importing Libraries and Project Modules for Linear Regression",
   "id": "476196c847f7f693"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import DataPrep"
   ],
   "id": "c0a051e1f7bde74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block imports all the necessary libraries and modules to perform linear regression modeling, evaluation and visualization - using built-in tools from 'scikit-learn' and 'DataPrep'",
   "id": "9100372067b7b649"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting label-encoded data for OLS regression",
   "id": "d22c621249dd17b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train_OLS, X_test_OLS, Y_train_OLS, Y_test_OLS = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)",
   "id": "435d381e8025002d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the label-encoded dataset into training and testing sets to prepare for training a Ordinary Least Square (OLS) regression model.",
   "id": "8a117ff84daed930"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Set model specs",
   "id": "b441d091749fce51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_train_OLS, Y_train_OLS)"
   ],
   "id": "a49e81de5fae0bb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates and fits an Ordinary Least Square linear regression model for the training data.\n",
    "1. Line1: Initializes a new instance of 'LinearRegression' from 'scikit-learn' which assumes a linear relationship between the input features and the target variable.\n",
    "2. Line2: Fits the model on the training dataset and learns the optimal coefficients (√ü) and intercept to minimize the sum of squared errors between predicted and actual values."
   ],
   "id": "8643c1810ba9d289"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Prediction and Result",
   "id": "81d2794e867b18b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_train_pred = ols_model.predict(X_train_OLS)\n",
    "y_test_pred = ols_model.predict(X_test_OLS)\n",
    "\n",
    "r2_train = r2_score(Y_train_OLS, y_train_pred)\n",
    "r2_test = r2_score(Y_test_OLS, y_test_pred)\n",
    "\n",
    "DataPrep.report.loc[len(DataPrep.report)] = ['OLS RegressionLC', r2_train, r2_test,np.sqrt(mean_squared_error(Y_test_OLS, y_test_pred)), \"\", \"\"]"
   ],
   "id": "fe32c0d718211a45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block generates predictions using the OLS model, calculates performance metrics and stores the results in the model comparison report ('DataPrep.report').\n",
    "1. Line1: Predicts the target variable for the training dataset using the fitted OLS model.\n",
    "2. Line2: Predicts the target variable for the test dataset using the fitted OLS model.\n",
    "3. Line4: Calculates the coefficient of determination (r2) for the training set.\n",
    "4. Line5: Calculates the coefficient of determination (r2) for the test set.\n",
    "5. Line7: Calculates Root Mean Squared Error (RMSE) for the test set and appends a row to the 'DataPrep.report' with the model name, r2 scores, RMSE scores."
   ],
   "id": "31f788617dc91218"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Residual plot based on OLS result",
   "id": "866c2a030623d604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "residuals = Y_test_OLS - y_test_pred\n",
    "plt.scatter(Y_test_OLS, residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Selling price\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual plot - OLS LE\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6e9ebabb2844d605"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block generates a residual plot which is a key diagnostic tool for evaluating the quality of a regression model.\n",
    "1. Line1: Calculates the residuals.\n",
    "2. Line2: Creates a scatter plot of redsiduals against the actual selling prices which helps to detect patterns that indicate model bias or non-linearity.\n",
    "3. Line3: Draws a horizontal red dashed line at residual = 0 -> representing perfect predictions; Points above the line -> underprediction; Points below the line -> overprediction\n",
    "4. Line4 - Line6: Adds axis names and proper layout\n",
    "5. Line7: Prevents overlapping elements\n",
    "6. Line8: Displays the plot"
   ],
   "id": "43b4d3a175b9c0a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Label Encoded Data",
   "id": "aadecc53d4d53245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train_OLS, X_test_OLS, Y_train_OLS, Y_test_OLS = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "47d12e2b0256f85a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the one-hot encoded dataset into training and testing sets to prepare for training and evaluating an OLS regression model.",
   "id": "15723d255a161411"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training an OLS regression model",
   "id": "9c6e1cbc7b9dc09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_train_OLS, Y_train_OLS)"
   ],
   "id": "1ee611be889f77d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates and trains an OLS linear regression model using one-hot encoded features as input\n",
    "1. Line1: Initiates a new linear regression model from 'scikit-learn'\n",
    "2. Line2: Trains the model on the training dataset"
   ],
   "id": "f3a0ab3759bf5080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluate and log performance of OLS regression",
   "id": "2d2f94f826565933"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_train_pred = ols_model.predict(X_train_OLS)\n",
    "y_test_pred = ols_model.predict(X_test_OLS)\n",
    "\n",
    "r2_train = r2_score(Y_train_OLS, y_train_pred)\n",
    "r2_test = r2_score(Y_test_OLS, y_test_pred)\n",
    "\n",
    "DataPrep.report.loc[len(DataPrep.report)] = ['OLS RegressionOH', r2_train, r2_test,np.sqrt(mean_squared_error(Y_test_OLS, y_test_pred)), \"\", \"\"]\n",
    "\n",
    "print(DataPrep.report.head())"
   ],
   "id": "f701e608869c720"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block generates predictions using the trained OLS model, computes performance metrics and appends the results to the centralized model comparison report.\n",
    "1. Line1: Predicts selling prices for training data using the OLS model.\n",
    "2. Line2: Predicts selling prices for test data using the OLS model.\n",
    "3. Line4: Calculates the coefficient of determination (r2) for the training set.\n",
    "4. Line5: Calculates the coefficient of determination (r2) for the test set.\n",
    "5. Line7: Calculates Root Mean Squared Error (RMSE) for the test set and appends a row to the 'DataPrep.report' with the model name, r2 scores, RMSE scores.\n",
    "6. Line9: Displays the first few rows of the report to confirm logging was successful."
   ],
   "id": "1daacc13ca09ae85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Residual plot for OLS regression",
   "id": "26983cfe8de8409e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "residuals = Y_test_OLS - y_test_pred\n",
    "plt.scatter(Y_test_OLS, residuals)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Selling price\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Residual plot - OLS OH\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6166b12092d2864f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a residual plot to visualize the prediction errors of the OLS regression model trained on one-hot encoded data.\n",
    "1. Line1: Calculates the residuals\n",
    "2. Line2: Creates a scatter plot of redsiduals against the actual selling prices which helps to detect patterns that indicate model bias or non-linearity.\n",
    "3. Line3: Draws a horizontal red dashed line at residual = 0 -> representing perfect predictions; Points above the line -> underprediction; Points below the line -> overprediction\n",
    "4. Line4 - Line6: Adds axis names and proper layout\n",
    "5. Line7: Prevents overlapping elements\n",
    "6. Line8: Displays the plot"
   ],
   "id": "b20014338692afd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest",
   "id": "b37d193bcdf756a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Importing Libraries for Random Forest Regression",
   "id": "6d503eb58016ef4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from JupiterPycharmProjekt import DataPrep"
   ],
   "id": "85bbb901d609e49a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code imports the necessary libraries and modules to build and optimize a RandomForest regression model using the prepared dataset.\n",
    "1. Line1: Imports the 'RandomForestRegressor' which combines many decision trees and averages their predictions to improve accuracy and reduce overfitting.\n",
    "2. Line2: Imports 'GridSearchCV' for hyperparameter tuning using cross-validation and 'train_test_split' to split the dataset into training and testing sets.\n",
    "3. Line3: Imports 'DataPrep'"
   ],
   "id": "ed5089f77bc3062"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining a hyperparameter grid for random forest regression",
   "id": "a0b893978b7e2e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "    'max_depth': [4, 5, 6, 7, 8],\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'criterion': ['squared_error', 'absolute_error']  # [Fix] g√ºltige Kriterien f√ºr RandomForestRegressor\n",
    "}"
   ],
   "id": "a1e8ea8930060a9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This defines a grid of hyperparameters for tuning a 'RandomForestRegressor' using grid search with cross-validation.\n",
    "1. Line1: Defines the maximum depth of each decision tree in the forest\n",
    "2. Line2: Indicates the number of tree's in the random forest\n",
    "3. Line3: Indicates the function used to measure the quality of a split in each tree"
   ],
   "id": "ee092fc5c8ff8706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train-Test split for Random Forest",
   "id": "8d2648f7947a48a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_RF, X_test_RF, Y_train_RF, Y_test_RF) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)",
   "id": "9e14bac453dccbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the label-encoded dataset into training and testing subsets for training a Random Forest regression model.",
   "id": "6fcafe027c2e0d83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Initialize Random Forest Regressor",
   "id": "d207752869aa7a83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "RForregCV = RandomForestRegressor(random_state=42)",
   "id": "85afbe9605881465"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a Random Forest Regressor instance with a fixed random seed for reproducibility.",
   "id": "4ad5a5db38ce5eae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####",
   "id": "90ee0f0e4718c28f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_rfmodel = GridSearchCV(estimator=RForregCV, param_grid=param_grid, cv=4, n_jobs=-1)\n",
    "CV_rfmodel.fit(X_train_RF, Y_train_RF)"
   ],
   "id": "429530d613dc36c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
