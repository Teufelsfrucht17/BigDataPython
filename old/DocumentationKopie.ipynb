{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Big Data Project",
   "id": "4de262cb655f9812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "9bbbb2abc54ac70d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import DataPrep\n",
    "import OLS_Regression\n",
    "import Ridge_Regression\n",
    "import SVR\n",
    "import NeuralNetworks\n",
    "import RandomForest\n",
    "import KNN\n",
    "\n",
    "print(DataPrep.report)\n",
    "import DataPrep\n",
    "\n",
    "import JupiterPycharmProjekt.OLS_Regression\n",
    "import JupiterPycharmProjekt.Ridge_Regression\n",
    "import JupiterPycharmProjekt.SVR\n",
    "import JupiterPycharmProjekt.NeuralNetworks\n",
    "import JupiterPycharmProjekt.RandomForest\n",
    "import JupiterPycharmProjekt.KNN\n",
    "\n",
    "print(DataPrep.report)"
   ],
   "id": "d984ea6ba7e0fc87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block executes all model training and evaluation scripts, collects their results and prints the final comparison report.",
   "id": "555d6b7178624879"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation  gerne nochmal checken glaube haben dort einiges verändert",
   "id": "267c4a5f3a557dcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### General\n",
    "In this section the environment for Data Preparation is set up by importing essential Python libraries. Each library plays a key role for the Data Preparation."
   ],
   "id": "b78022a041fcf536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ],
   "id": "89e05167b6e7931a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "pandas:             Used for handling and analyzing structured data\n",
    "\n",
    "matplotlib.pyplot:  A fundamental plotting library.\n",
    "\n",
    "seaborn:            Built on top of matplotlib and simplifies the process of graphical statistics.\n",
    "\n",
    "sklearn.model_selection + train_test_split: Helps to split a dataset into training and test dataset.\n",
    "\n",
    "sklearn.preprocessing + LabelEncoder: Transforming data before feeding it into a model.\n",
    "\n",
    "numpy: Is the foundational package for numerical computing in Python."
   ],
   "id": "2d852d3d7316d4c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read original data from CSV",
   "id": "9cc27d4c329134a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = pd.read_csv('../JupiterPycharmProjekt/UsedCarSellingPrices.csv')",
   "id": "3b012ca4ae967562"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code uses pandas to read the CSV file \"Used Car Selling Prices\" and loads it into a dataframe called 'data'.",
   "id": "a2f4da421a209ea9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label-Encoding for Visualisation before cleaning",
   "id": "9cf5b9b282e005bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define columns that need to be lable-encoded",
   "id": "608d640c1fe43724"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']",
   "id": "41a24ce99dbdfa90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This line defines a list of column names that represent categorical features in the dataset.\n",
    "\n",
    "fuel: type of fuel that is used by the car\n",
    "\n",
    "seller_type: type of car seller\n",
    "\n",
    "transmission: type of gear\n",
    "\n",
    "owner: status of ownership"
   ],
   "id": "b158b71c043ad55b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Copy data into var label_encoded_data; create empty array lable_encoders",
   "id": "fa5881cba3acbbe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "label_encoded_data = data.copy()\n",
    "label_encoders = {}"
   ],
   "id": "bd05bd955bf47296"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code sets up the environment for label encoding.\n",
    "\n",
    "Line1: Copying the dataset\n",
    "\n",
    "Line2: Initializing the Encoders Dictionary"
   ],
   "id": "86bc5c2561dc4409"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run lable-encoder for every previosly defined columne (function imported from sklearn)",
   "id": "2d1117b147382206"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    label_encoded_data[col] = le.fit_transform(label_encoded_data[col])\n",
    "    label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(\"\\nLabel-Encoded Data for Visualisation:\")\n",
    "print(label_encoded_data.head())"
   ],
   "id": "fa44f5e1c507bbfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This loop iterates over each categorical column and applies Label Encoding transforming string labels into numeric codes.\n",
    "\n",
    "1. for col in categorical_columns: Loops through each column listed earlier\n",
    "2. le = labelEncoder(): Creates a new LabelEncoder instance from scikit-learn for the current column\n",
    "3. label_encoded_data[col] = le.fit_transform(label_encoded_data[col]): Fits the encoder to the column's categories and transforms them into integers + Replaces the original text values in label_encoded_data with the corresponding numeric labels\n",
    "4. label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_))): Stores the mapping of original category names to their encoded values in the label_encoders dictionary + This allows to trace or reverse the encoding later if needed"
   ],
   "id": "a53b8f263b7afdac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### concatinates x and y into one point to be visualized",
   "id": "b8e6900896de12da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = pd.concat([label_encoded_data], axis=1)",
   "id": "6544471d41d7c0f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new DataFrame called 'all_data_LableEncoded' by concatenating 'label_encoded_data' along the column axis 'axis=1'",
   "id": "a8672f704c4ed8cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Drops all columes that are non-numeric to make scaling possible",
   "id": "a4aa20a41c9d4d18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = all_data_LableEncoded.select_dtypes(include=['number'])",
   "id": "9b44ad7e42021698"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line filters the dataset to keep only the numeric columns from 'all_data_LableEncoded'.",
   "id": "d5c772213e52f3d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scale data (normalized via MinMaxScaler - between 0 and 1)\n",
    "sscaler = preprocessing.StandardScaler()    ???\n",
    "\n",
    "all_data_LableEncoded = sscaler.fit_transform(all_data_LableEncoded)    ???"
   ],
   "id": "1479907ca0419ea7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nscaler = preprocessing.MinMaxScaler()\n",
    "all_data_LableEncoded = nscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "7280e14fd343ee53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs Min-Max Scaling on the numeric features in the dataset, transforming them into a commonscale between 0 and 1.\n",
    "1. Line1: Initalizes a MinMaxScaler object from scikit-learn\n",
    "2. Line2: Calculates the min and max values for each feature iin the dataset and applies the scaling transformation to each value"
   ],
   "id": "b4cfe17fcb90f1f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualisation before cleaning Data",
   "id": "be1db059d65049e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reintegrates Column name for boxplot",
   "id": "d51fdd782386be4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scaled_df = pd.DataFrame(all_data_LableEncoded, columns=label_encoded_data.select_dtypes(include='number').columns)",
   "id": "90d8f4767542a5a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line converts the scaled NumPy array (from the Min-Max Scaler) back into a pandas DataFrame and restores the original column names.",
   "id": "db5d562516835ad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Boxplot with readable x-axis",
   "id": "bb91a96b644372aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=scaled_df, orient='v', palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Normed boxplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1b5bfa8286be97e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot for each feature in the 'scaled_df' DataFrame to visualize the distribution and spread of the normalized (Min-max scaled) data.\n",
    "1. Line1: Sets the size of the figure to be 12 inches wide by 6 inches tall and ensures the plot is large enough to accommodate all features without crowding.\n",
    "2. Line2: Creates a vertical boxplot for each column in the 'scaled_df' DataFrame and uses Seaborn's elegant and color-friendly 'Set2' palette. Each box shows the median, the IQR and Whiskers & Outliers.\n",
    "3. Line3: Rotates the x-axis labels by 45 degrees for better readability, especially when there are many features.\n",
    "4. Line4: Adds a title to the plot for context, signaling that the data is normalized.\n",
    "5. Line5: Adjusts spacing to prevent overlap between axis labels, titles, and plot content.\n",
    "6. Line6: Renders and displays the final plot in the notebook."
   ],
   "id": "4c324b4595cd2bc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Boxplot for only numerical data",
   "id": "4d6d06f58906f42e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_cols = ['selling_price', 'km_driven', 'year']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=scaled_df[selected_cols], orient='v', palette='Set3')\n",
    "plt.title(\"Normed boxplot for numerical data only\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8436caaba0a27fa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This blocks generates a boxplot visualization focused on three specific, scaled numerical features:\n",
    "\n",
    "'selling_price'\n",
    "\n",
    "'km_driven'\n",
    "\n",
    "'year'\n",
    "\n",
    "1. Line1: Selects the subset of important numerical features for focused analysis.\n",
    "2. Line2: Sets the plot size to be 8 inches wide and 5 inches tall.\n",
    "3. Line3: Creates a vertical boxplot for just the selected columns using the soft, pastel 'Set3' color palette from Seaborn.\n",
    "4. Line4: Adds a descriptive title to clarify that this plot shows normalized (scaled) numerical features.\n",
    "5. Line5: Ensures layout is adjusted for neatness and then displays the plot."
   ],
   "id": "4b53740f46800766"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pairplot to show correlation",
   "id": "732bcfbf412b2876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.pairplot(scaled_df[selected_cols])\n",
    "plt.suptitle(\"Pairplot for select charactaristics\", y=1.02)\n",
    "plt.show()"
   ],
   "id": "f209c747959ecd64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This blocks creates a pairplot to visually explore pairwise relationships among the selected numerical features:\n",
    "\n",
    "'selling_price'\n",
    "\n",
    "'km_driven'\n",
    "\n",
    "'year'\n",
    "\n",
    "1. Line1: Creates a grid of scatterplots for each pairwise combination of the selected features. This helps to visualize Correlations, Clustering tendencies and Linearity or Non-Linearity Relationships. The Histograms are shown on the diagonal to represent each variable's distribution.\n",
    "2. Line2: Adds a super title above the entire plot grid.\n",
    "3. Line3: Renders the entire pairplot for viewing."
   ],
   "id": "732cafe36aff97ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean Data & Create variable Brand",
   "id": "89b140c4d98393d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove missing data",
   "id": "5d63e1a92ab5b546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = data.dropna()",
   "id": "da4f98bbc2d5a2a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line removes all rows with missing values from the 'data' DataFrame.",
   "id": "e073a152d043b57b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Show how much data was removed",
   "id": "e7e8a855cc2547ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Data after removing data:\")\n",
    "print(data.isnull().sum())\n",
    "print(f\"Remaining rows: {len(data)}\")"
   ],
   "id": "33fb8d9b7262c1ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block checks and confirms that all missing values have been removed from the dataset and reports the number of remaining rows.\n",
    "\n",
    "1. Line1: Prints a header to indicate that the following output relates to the cleaned dataset.\n",
    "2. Line2: Checks for missing values in each column of the 'data' DataFrame, creates a Boolean mask of the same shape as the data and then counts the number of 'True' values in each column, i.e. the number of missing entries.\n",
    "3. Line3: Prints the total number of rows left in the dataset after dropping rows with missing values using 'len(data)'."
   ],
   "id": "e0498ff9c71e3ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### IQR-based Removal of Outliers",
   "id": "67778762708c2c56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_outliers_iqr(df, column):\n",
    "    Q1 = np.percentile(df[column], 25)\n",
    "    Q3 = np.percentile(df[column], 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]"
   ],
   "id": "593a83011049a5bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function removes outliers from a specific column in a DataFrame using the Interquartile Range (IQR) method\n",
    "\n",
    "1. 'Q1': 25th percentile - the value below which 25% of the data falls\n",
    "2. 'Q3': 75th percentile - the value below which 75% of the data falls\n",
    "3. 'IQR = Q3-Q1': IQR is the spread of the middle 50% of values and it is used to understand the natural range of variation in the data.\n",
    "4. Line5 + Line6: These define the acceptable range and any values below the lower bound or above the upper bound are considered outliers.\n",
    "5. Line7: Returns a filtered version of the original DataFrame, keeping only the rows where the specified column's value is within the acceptable range -> Outlier are removed."
   ],
   "id": "6feb10562aee8ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Apply to most important column",
   "id": "c8d59d5edfd017a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = remove_outliers_iqr(data, 'selling_price')\n",
    "data = remove_outliers_iqr(data, 'km_driven')"
   ],
   "id": "ddf4aa0260275fed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These lines apply the IQR-based outlier removal function to two important columns in the dataset: 'selling_price' and 'km_driven'.\n",
    "\n",
    "1. Line1: Removes rows where 'selling_price' is considered an outlier based on the IQR rule and keeps only cars with selling prices within the typical range.\n",
    "2. Line2: Applies the same IQR filtering to the 'km_driven' column and eliminates unusually low or high mileage entries that could distort statistical analysis or model training."
   ],
   "id": "b2e9962dbdeac101"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nDaten nach IQR-basierter Ausreißerbereinigung:\")\n",
    "print(f\"Max. Verkaufspreis: {data['selling_price'].max()}\")\n",
    "print(f\"Max. Kilometerstand: {data['km_driven'].max()}\")\n",
    "print(f\"Verbleibende Zeilen nach IQR-Filter: {len(data)}\")"
   ],
   "id": "9eb3b5e357e0e471"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block prints a quick summary of the dataset after removing outliers using the IQR method.\n",
    "1. Line1: Prints a headline used for clarity when reading the console output.\n",
    "2. Line2: Displays the maximum selling price in the cleaned dataset which helps verifying that extremely high prices have been removed.\n",
    "3. Line3: Shows the maximum odometer reading after outlier removal and ensures that unusally high mileage values have been filtered out.\n",
    "4. Line4: Prints the number of remaining rows in the dataset which tells how much data is left after removing rows that contained outliers in 'selling_price' and 'km_driven'."
   ],
   "id": "61e2f065ad42028f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create 'Brand' as new column",
   "id": "d49e37cb539be166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data['brand'] = data['name'].str.split().str[0]\n",
    "print(data)"
   ],
   "id": "386d3bebe0f3f10e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new column called 'brand' by extracting the first word from the 'name' column which represents the car brand.",
   "id": "42aad8125cdf5cd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Label-Encoding for Visualisation after cleaning",
   "id": "e026e6e917f80000"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ???",
   "id": "9f383cff6b7b49eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "categorical_columns = ['fuel', 'seller_type', 'transmission', 'owner']\n",
    "label_encoded_data = data.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    label_encoded_data[col] = le.fit_transform(label_encoded_data[col])\n",
    "    label_encoders[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(\"\\nLabel-Encoded Data (nur zur Referenz):\")\n",
    "print(label_encoded_data.head())"
   ],
   "id": "ed6e8e8972ea2d63"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs label encoding on selected categorical columns, converting them from text values to integers so they can be used in regression models.\n",
    "\n",
    "1. Line1: Specifies the list of categorical features to be encoded which are typically textual descriptors that must be converted into numeric format for modeling.\n",
    "2. Line2: Creates a copy of the original dataset to apply the encodings without altering the raw data.\n",
    "3. Line3: Initializes an empty dictionary to store the encoding mappings for each categorical column.\n",
    "4. The 'for' loop: Iterates over each column in the 'categorical_columns' list; The 'LabelEncoder()' from scikit-learn is used to convert category labels into integers. The transformed values replace the original column in 'label_encoded_data'. The mapping of original class labels to integer codes is stored in 'label_encoders' for reference or inverse transformation later.\n",
    "5. Line10 + Line11: Displays the first few rows of the updated dataset to confirm that the categorical features have been encoded."
   ],
   "id": "c013de39fa21bccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### concatinate x and y into one point to be visualized",
   "id": "c751e8b5bc2b2ddc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = pd.concat([label_encoded_data], axis=1)",
   "id": "575c260ccb086891"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates a new DataFrame called 'all_data_LableEncoded' by concatenating the contents of 'label_encoded_data' along the columns axis.",
   "id": "a0c0ea74921d9c3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Drop all columns that are non-numeric to make scaling possible",
   "id": "968c09bb308f2730"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_data_LableEncoded = all_data_LableEncoded.select_dtypes(include=['number'])",
   "id": "7e62806eaf50737f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line filters the 'all_data_LableEncoded' DataFrame to include only numeric columns, removing any that are not numeric.",
   "id": "4e52f5cc6dafece8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Scale data (normalized)\n",
    "\n",
    "sscaler = preprocessing.StandardScaler()\n",
    "\n",
    "all_data_LableEncoded = sscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "a387af038b48cbe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nscaler = preprocessing.MinMaxScaler()\n",
    "all_data_LableEncoded = nscaler.fit_transform(all_data_LableEncoded)"
   ],
   "id": "e00438e2eee09378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code applies Min-Max Scaling to normalize all numeric features in the dataset, transforming their values to a common range between 0 and 1.\n",
    "1. Line1: Initializes a MinMaxScaler object from scikit-learn's 'preprocessing' module and will scale each feature individually.\n",
    "2. Line2: Calculates the minimum and maximum values for each feature and scales each value in the dataset to the 0-1 range."
   ],
   "id": "65c54ee5015a952a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization after Data cleaning",
   "id": "2c6515be8ef29c26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scale Data ???",
   "id": "5187aa1e6e968205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "scaled_df = pd.DataFrame(all_data_LableEncoded, columns=label_encoded_data.select_dtypes(include='number').columns)",
   "id": "3007d93fa855f241"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line converts the scaled NumPy array back into a pandas DataFrame and restores the original column names, making the data human-readable and easier to work with.",
   "id": "e62e77c088f70ad3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1. Boxplot with readable axis names",
   "id": "76f5b2d684de4f11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=scaled_df, orient='v', palette='Set2')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot der skalierten numerischen Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "277090bde61a92a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot for all features in the 'scaled_df' DataFrame, which contains only scaled numeric data. The plot helps visually inspecting the distribution and variability of each feature.\n",
    "1. Line1: Sets the figure size to 12 inches wide by 6 inches tall for better readability.\n",
    "2. Line2: Uses Seaborn to draw vertical boxplots for each numeric feature in 'scaled_df' and 'palette=Set2' gives the plot a soft, color-coded appearance to distinguish features visually.\n",
    "3. Line3: Rotates the x-axis labels by 45 degrees so that long feature names dont overlap and remain legible.\n",
    "4. Line4: Adds a descriptive title\n",
    "5. Line5 + Line6: Adjusts the layout to avoid overlapping elements and displays the final plot."
   ],
   "id": "eb04ac46c505c09c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Boxplot only for selected numeric columns",
   "id": "be8c18a65f2ed5b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "selected_cols = ['selling_price', 'km_driven', 'year']\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=scaled_df[selected_cols], orient='v', palette='Set3')\n",
    "plt.title(\"Boxplot ausgewählter Merkmale\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f411c6b86c61ca0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a boxplot visualization for a selected subset of key numeric features: 'selling_price', 'km_driven' and 'year', all of which have been previously scaled to 0-1 range.\n",
    "1. Line1: Selects the three features for targeted visualization.\n",
    "2. Line2: Sets the figure size to 8 inches wide and 5 inches tall for compact clarity.\n",
    "3. Line3: Draws vertical boxplots for just the selected columns using Seaborn's pastel 'Set3' color palette.\n",
    "4. Line4: Adds a title.\n",
    "5. Line5 + Line6: Adjusts spacing to prevent overlap and displays the plot."
   ],
   "id": "4b99832b469b318f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Pairplot for Distribution and Correlation",
   "id": "787fd27f2aa7dce2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.pairplot(scaled_df[selected_cols])\n",
    "plt.suptitle(\"Paarweise Verteilungen ausgewählter Merkmale\", y=1.02)\n",
    "plt.show()\n",
    "#print(\"Trainingsdaten zusätzlich als 'prepared_used_car_data_train.parquet' gespeichert.\")\n",
    "#print(\"Testdaten zusätzlich als 'prepared_used_car_data_test.parquet' gespeichert.\")\n",
    "#print(\"Gesamtdaten zusätzlich als 'prepared_used_car_data.parquet' gespeichert.\")"
   ],
   "id": "c0067e307d64278c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates a pairplot that visualizes the pairwise relationships and distributions of three selected, scaled features: 'selling_price', 'km_driven' and 'year'.\n",
    "1. Line1: Generates a grid of plots: Scatter plots and Histograms.\n",
    "2. Line2: Adds a descriptive title above the plot grid; 'y=1.02' adjusts the title position slightly above the plot area to prevent overlap.\n",
    "3. Line3: Renders and displays the plot."
   ],
   "id": "96d27fc6134f39a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### One-Hot-Encoding for Regression Model Training & Testing",
   "id": "cc75a4b0848e8a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### One-Hot-Encoding for categorical Variables",
   "id": "ebc53343396929a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoded_data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(\"\\nOne-Hot-Encoded Data:\")\n",
    "print(encoded_data.head())\n",
    "\n",
    "print(encoded_data)"
   ],
   "id": "67d6e2a85645fa42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block uses one-hot encoding to transform categorical columns in the dataset into binary 0 or 1 columns, making them suitable for regression models.\n",
    "1. Line1: Performs one-hot encoding on the columns listed in 'categorical_columns'. For each unique category in these columns, new binary columns are created. Drops the first category for each column to avoid multicollinearity when using models like linear regression.\n",
    "2. Line3: Prints a header for clarity in console output.\n",
    "3. Line4: Displays the first few rows of the encoded dataset for a quick preview.\n",
    "4. Line6: Prints the entire DataFrame, which now contains both numeric and one-hot encoded binary columns."
   ],
   "id": "8b4293aacecedf34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sort data by 'year' and 'km_driven'",
   "id": "45c74f0fa5c1b306"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "encoded_data_sorted = encoded_data.sort_values(by=['year', 'km_driven'], ascending=[False, True])",
   "id": "45550ed02db0c8f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line sorts the encoded dataset based on two columns - 'year' and 'km_driven' - to organize the data in a meaningful order.",
   "id": "ed2a1562262008b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare data used for regression analysis",
   "id": "51dc1c4288e1a263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features = encoded_data_sorted.columns.drop(['name', 'selling_price'])\n",
    "target = 'selling_price'\n",
    "\n",
    "X = encoded_data_sorted[features]\n",
    "y = encoded_data_sorted[target]"
   ],
   "id": "b32f5eb379e21c69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block prepares the feature matrix 'X' and target vector 'Y' for training a regression model, using sorted and one-hot encoded dataset.\n",
    "1. Line1: Selects all column names except: 'name' and 'selling_price' -> Result is a list of input features for the model.\n",
    "2. Line2: Explicitly defines 'selling_price' as the target variable.\n",
    "3. Line4: Creates the feature matrix 'X' by selecting only the columns in 'features' from the dataset; 'X' will be used as input for the regression model\n",
    "4. Line5: Creates the target vector 'Y', which contains the selling prices (values to be predicted)"
   ],
   "id": "e505e7c96414a817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving test data",
   "id": "9b1c9c09836a4a3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "all_data = pd.concat([X, y], axis=1)\n",
    "all_data.to_csv('prepared_used_car_data_all.csv', index=False)"
   ],
   "id": "1db972d525904611"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block recombines the feature 'X' and target 'Y' into a single DataFrame and saves it as a '.csv' file for future use.\n",
    "1. Line1: Concatenates the feature matrix 'X' and the target vector 'Y' horizontally and reconstructs the full dataset 'all_data' with both inputs and outputs in one table.\n",
    "2. Line2: Saves the combined dataset to a CSV file and ensures that the DataFrame index is not written to the file, keeping the output clean and suitable for reuse."
   ],
   "id": "bff60c48d8027a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creation of training and test data",
   "id": "e45bd2d900b15339"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "id": "adc4bd0cd63e155b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line uses scikit-learn's 'train_test_split()' function to divide the dataset into training and testing subsets, a crucial step for evaluating regression models.",
   "id": "35d5ef6c097b7bd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving of training data",
   "id": "acdef0c8dcc14af4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data.to_csv('prepared_used_car_data_train.csv', index=False)"
   ],
   "id": "efa32767bb7c9979"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block combines the training features and labels into a single DataFrame and then exports it to a '.csv' file for storage or reuse.\n",
    "1. Line1: Merges the training input features 'X_train' and then training target values 'Y_train' side by side (along columns) and produces a single DataFrame 'train_data' that contains all the necessary data for training a model.\n",
    "2. Line2: Saves the 'train_data' DataFrame as a CSV file and ensures the row indices are not written into the file, keeping it clean and easy to reload."
   ],
   "id": "df716cf15fcd172"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving of test data",
   "id": "130f62c7c5af4cf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data.to_csv('prepared_used_car_data_test.csv', index=False)"
   ],
   "id": "864a111dd049d481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code combines the test features and labels into a single DataFrame and then saves it as a CSV file for future use or evaluation.\n",
    "1. Line1: Merges the test feature set 'X_test' and the corresponding target values 'Y_test' horizontally and produces a new DataFrame 'test_data' that includes all the columns needed to evaluate a regression model.\n",
    "2. Line2: Saves the resulting test dataset to a CSV file and prevents the row index from being included in the file, making the CSV clean and readable."
   ],
   "id": "8091dbfbcd607591"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confirming saved files",
   "id": "9e643d64786ea4de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nTrainingsdaten gespeichert als 'prepared_used_car_data_train.csv'\")\n",
    "print(\"Testdaten gespeichert als 'prepared_used_car_data_test.csv'\")"
   ],
   "id": "68ba207c5efe6c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These print statements simply confirm to the user that the training and test datasets have been successfully saved to CSV files.\n",
    "1. Line1: Outputs a message confirming that the training data was saved.\n",
    "2. Line2: Confirms that the test data was also saved."
   ],
   "id": "d0d43b1e32a3fd6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KNN",
   "id": "3e6f5677d1308cad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Set Up",
   "id": "310a1040c0a6155b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import DataPrep\n",
    "\n",
    "(X_train_KNN, X_test_KNN, Y_train_KNN, Y_test_KNN) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)"
   ],
   "id": "c5be736cfe574576"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block sets upo the environment to train and evaluate a K-Nearest Neighbors Regression model using a train/test split of preprocessed data.\n",
    "\n",
    "1. Line1: Imports the KNN regressor from scikit-learn\n",
    "2. Line2: Adds a tool for hyperparameter tuning by trying different values and splits the dataset into training and testing sets\n",
    "3. Line3: Imports the DataPrep file\n",
    "4. Line5: Splits features and target into a training set and testing set while ensuring that the split is reproducible"
   ],
   "id": "fe80564ebe96f48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Initialize KNN model",
   "id": "8373151cac88653f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "knnmodelCV = KNeighborsRegressor()",
   "id": "c72608e6b83f9ff8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line creates an instance of the KNN model from scikit-learn with default parameters.",
   "id": "4101a93bef15f9cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define grid of neighbor counts",
   "id": "83ec9ddf494c160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "'n_neighbors': range(3, 22, 2),\n",
    "'weights': ['uniform', 'distance'],\n",
    "'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "'leaf_size': [8, 16, 32, 64, 128, 256, 512],\n",
    "'p': [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "}"
   ],
   "id": "d1d50790ac5e12f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block defines a hyperparameter grid that will be used to tune a KNN regressor using tools like GridSearchCV.\n",
    "\n",
    "1. Line2: Tries odd values for k from 3 to 21 which defines how many neighbors the model considers when making predictions.\n",
    "2. Line3: 'uniform' makes sure all neighbors contribute equally to the prediction and 'distance' makes sure that closer neighbors contribute more to the prediction than distant ones.\n",
    "3. Line4: Specifies the search algorithm used to find neighbors. 'auto' -> Chooses the best algorithm based on the data. 'ball_tree' and 'kd_tree' -> Use tree structure for fast searches. 'brute' -> Calculates distances directly.\n",
    "4. Line5: Affects the speed vs. memory tradeoff in tree based algorithms. Smaller values result in faster query time. Larger values result in faster tree building.\n",
    "5. Line6: Specifies the power parameter for the Minkowski distance."
   ],
   "id": "c5ca43f727efcf8f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run 10-fold cross-validation across neighbor settings",
   "id": "bb7aa1526407a7dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_knnmodel = GridSearchCV(estimator=knnmodelCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_knnmodel.fit(X_train_KNN, Y_train_KNN)"
   ],
   "id": "ffbca94501468e40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code uses GridSearchCV from scikit-learn to perform an exhaustive search over a defined set of hyperparameters for a KNN regression model.\n",
    "Assigns all available CPU cores to parallelize the search and speed up computation and uses 10-fold cross-validation."
   ],
   "id": "433585525cb7453b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output the best number of neighbors",
   "id": "ce89bb67c469c17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Best parameters found:\", CV_knnmodel.best_params_)",
   "id": "7f3422991e960229"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line prints the optimal set of hyperparameters that were found during the GridSearchCV process.",
   "id": "aeb7906c98048d88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating Model Performance",
   "id": "dbeeec75f6a80f7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_knnmodel.predict(X_train_KNN)\n",
    "Y_train_dev = sum((Y_train_KNN - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_KNN - Y_train_KNN.mean())**2)\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev"
   ],
   "id": "71ff168211e54ce1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually calculates the coefficient of determination (r2) to evaluate how well the KNN model fits the training data.\n",
    "1. Line1: Uses the trained cross-validated KNN model to predict selling prices for the training set.\n",
    "2. Line2: Calculates the residual sum of squares (RSS) and measures how far off the predictions are from the actual values which should be minimal.\n",
    "3. Line3: Calculates the total sum of squares (TSS) and measures the total variation in the target variable (how far are values from the mean)\n",
    "4. Line4: Calculates the coefficient of determination (r2)."
   ],
   "id": "c538e14dcca4264c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict on test set",
   "id": "22f9c72aa8d70365"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_knnmodel.predict(X_test_KNN)\n",
    "Y_test_dev = sum((Y_test_KNN - Y_test_pred)**2)\n",
    "Y_test_meandev = sum((Y_test_KNN - Y_test_KNN.mean())**2)\n",
    "pseudor2 = 1 - Y_test_dev / Y_test_meandev\n",
    "\n",
    "DataPrep.report.loc[len(DataPrep.report)] = [\"KNN_LE \", r2, pseudor2,\"\", CV_knnmodel.cv_results_['mean_test_score'][CV_knnmodel.best_index_], CV_knnmodel.cv_results_['std_test_score'][CV_knnmodel.best_index_]]"
   ],
   "id": "b681d58572a58c5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block evaluates the KNN model on the test set using the coefficient of determination and calculates a pseudo r2 score for generalization performance and appends a summary row to report DataFrame for later comparison with other models.\n",
    "1. Line1: Uses the best tuned KNN model to predict the selling prices for the test set.\n",
    "2. Line2: Calculates the residual sum of squares (RSS) on the test data.\n",
    "3. Line3: Computes the total sum of squares (TSS) of the actual test values\n",
    "4. Line4: Calculates pseudo-r2 which is the models explanatory power on the unseen test data.\n",
    "5. Line6: Logs results to a report."
   ],
   "id": "ead6a03f06364fd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Encoded Data for Training and Testing",
   "id": "c5abfc2edeadd30d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train_KNN, X_test_KNN, Y_train_KNN, Y_test_KNN) = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "4e60f52b842c744c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the one-hot encoded dataset into training and testing sets for building and evaluating a KNN model.",
   "id": "ec6157147d1b10dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Run 10-fold cross-validation across neighbor settings",
   "id": "40174a8e8fd2cf29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_knnmodel = GridSearchCV(estimator=knnmodelCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_knnmodel.fit(X_train_KNN, Y_train_KNN)"
   ],
   "id": "43437fc71a6c1106"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block performs hyperparameter optimization for the KNN regression model using one-hot encoded data with cross validation to ensure reliable model selection.\n",
    "1. Line1: Performs an exhaustive grid search over the parameter combinations defined in 'param_grid'.\n",
    "2. Line2: Trains and evaluates all hyperparameter combinations using the training data based on one-hot encoded features."
   ],
   "id": "d50a4a55c15536d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output the best number of neighbors",
   "id": "e3aadec7ca733760"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Best parameters found:\", CV_knnmodel.best_params_)\n",
    "Y_train_pred = CV_knnmodel.predict(X_train_KNN)\n",
    "Y_train_dev = sum((Y_train_KNN - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_KNN - Y_train_KNN.mean())**2)\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev"
   ],
   "id": "a959f29a03b9afc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This line prints the optimal hyperparameter combination selected by 'GridSearchCV' for the KNN regression model trained on one-hot encoded data.\n",
    "1. Line2: Predicts the selling prices for the training data using the best-found KNN model.\n",
    "2. Line3: Computes the residual sum of squares (RSS) and measures how much error remains after using the models predictions.\n",
    "3. Line4: Computes the total sum of squares (TSS) and represents the total variance in the training labels serving as a baseline.\n",
    "4. Line5: Calculates the r2 score which explains how much variation is explained by the model."
   ],
   "id": "a6dc2df3e05df1ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict on test set",
   "id": "a1b02c12638c451b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_knnmodel.predict(X_test_KNN)\n",
    "Y_test_dev = sum((Y_test_KNN - Y_test_pred)**2)\n",
    "Y_test_meandev = sum((Y_test_KNN - Y_test_KNN.mean())**2)\n",
    "pseudor2 = 1 - Y_test_dev / Y_test_meandev"
   ],
   "id": "a54a5b6c2af4e2ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block evaluates how well the tuned KNN model generalizes to unseen data by manually calculating the pseudo r2 score.\n",
    "1. Line1: Uses the trained KNN model to predict selling prices for the test set.\n",
    "2. Line2: Computes the residual sum of squares (RSS) for the test set and measures the models prediction error on unseen data.\n",
    "3. Line3: Compues the total sum of squares (TSS) for the test set and represents the total variance in the actual test values serving as a baseline.\n",
    "4. Line4: Calculates the pseudo-r2 and indicates how well the model generalizes to new unseen data."
   ],
   "id": "1ae7c6e6e5109f2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Log KNN Model results in a report",
   "id": "cf8b65d68bc352f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DataPrep.report.loc[len(DataPrep.report)] = [\"KNN_OH \", r2, pseudor2,\"\", CV_knnmodel.cv_results_['mean_test_score'][CV_knnmodel.best_index_], CV_knnmodel.cv_results_['std_test_score'][CV_knnmodel.best_index_]]\n",
    "print(DataPrep.report.head())\n",
    "print(CV_knnmodel.best_params_)"
   ],
   "id": "e6df16464e8f85b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block logs the performance metrics of the KNN model trained with one-hot encoded data into a centralized evaluation report and then displays the report and the best hyperparameters.",
   "id": "1d5aca7ec4aa0edd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Neural Networks",
   "id": "25f581d8e8b1ddb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preparation to train a Neural Network",
   "id": "8bc59d768143c993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import DataPrep"
   ],
   "id": "9672b2f3b47dc53f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block sets up the environment for training and tuning a neural network regression model using scikit-learn's 'MLPRegressor' with data and configuration coming from a shared module called 'DataPrep'.\n",
    "1. Line1: Used to split data into training and testing sets and enables hyperparameter tuning using exhaustive search with cross-validation.\n",
    "2. Line2: Imports 'MLPRegressor' a multilayer perceptron for regression tasks.\n",
    "3. Line3: Imports the 'DataPrep' module."
   ],
   "id": "b94cc96a420ee1f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting Label-Encoded Data",
   "id": "2c81f36f0936057e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_nn, X_test_nn, Y_train_nn, Y_test_nn) = train_test_split(DataPrep.X_LE, DataPrep.Y_LE, test_size=0.2, random_state=42)",
   "id": "60c6a412c2819f7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the label-encoded dataset into training and testing sets preparing it for use in training a neural network regression model.",
   "id": "b43dea6ff21ea3cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining a Hyperparameter Grid for Tuning the Network Regressor",
   "id": "b5ff6cf2f63140b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid = {\n",
    "'hidden_layer_sizes': [(5,), (8,), (10,), (13,)],\n",
    "'alpha': [0.0, 0.0025, 0.005, 0.0075, 0.01, 0.1],\n",
    "'activation': ['logistic', 'tanh', 'relu'],\n",
    "'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "'max_iter': [5000],\n",
    "'random_state': [0],\n",
    "'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "}"
   ],
   "id": "b527f8d0b74c534f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block defines a parameter grid to be used with 'GridSearchCV' for tuning a 'MLPRegressor' model in a regression task. Each key represents a hyperparameter and each value is a list of options that 'GridSearchCV' will explore during cross-validation.\n",
    "1. Line2: Defines the structure of the hidden layers; how many neurons\n",
    "2. Line3: L2 regularization term which helps to prevent overfitting by penalizing large weights\n",
    "3. Line4: Specifies the activation function used in the hidden layers\n",
    "4. Line5: Optimization algorithm used for training\n",
    "5. Line6: Sets the maximum number of training iterations to 5000\n",
    "6. Line7: Ensures reproducibility by fixing the random seed used inernally by the model\n",
    "6. Line8: Controls how the learning rate adapts over time when using 'sgd' or 'adam'"
   ],
   "id": "5f678978f999a9c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training a Neural Network with Grid Search and Cross-Validation",
   "id": "a3f312196c4b01c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NNetRregCV = MLPRegressor()\n",
    "CV_nnmodel = GridSearchCV(estimator=NNetRregCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_nnmodel.fit(X_train_nn, Y_train_nn)"
   ],
   "id": "dfa3b9ef94572087"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block creates, tunes and trains a neural network regressor using scikit-learn's 'MLPRegressor' and 'GridSearchCV'.\n",
    "1. Line1: Initializes a Multi-Layer Perceptron (MLP) Regressor which is a type of feedforward neural network.\n",
    "2. Line2: Wraps the neural network model with 'GridSearchCV' to perform automated hyperparameter tuning.\n",
    "3. Line3: Trains the model using the training data and runs the grid search."
   ],
   "id": "7a922825fce0e80b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the Neural Network on Training Data",
   "id": "2ee6ba67bf982360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_nnmodel.predict(X_train_nn)\n",
    "Y_train_dev = sum((Y_train_nn - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_nn - Y_train_nn.mean())**2)  # [aus PDF]\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev  # [aus PDF]"
   ],
   "id": "46bbd393749f5ea9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually calculates the r2 score to evaluate how well the trained neural network model fits the training data.\n",
    "1. Line1: Uses the best neural network model selected by 'GridSearchCV' to predict target values for the training set\n",
    "2. Line2: Calculates the residual sum of squares (RSS) and measures the total prediction error of the model\n",
    "3. Line3: Calculates the total sum of squares (TSS) and measures how much variance is in the target data without any model\n",
    "4. Line4: Computes the r2 score which explains the variance in the training data"
   ],
   "id": "b3e9de294a60287a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating the Neural Network on Test Data",
   "id": "a0e2b78c7ab3800d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_nnmodel.predict(X_test_nn)\n",
    "Y_test_dev = sum((Y_test_nn - Y_test_pred)**2)\n",
    "Y_train_meandev = sum((Y_test_nn - Y_test_nn.mean())**2)  # [aus PDF]\n",
    "pseudor2 = 1 - Y_test_dev / Y_train_meandev"
   ],
   "id": "f1414605e9a76ac0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block calculates the pseudo r2 score to assess how well the trained neural network regressor performs on unseen test data.\n",
    "1. Line1: Predicts target values for the test set using the neural network model selected by 'GridSearchCV'\n",
    "2. Line2: Computes the residual sum of squares (RSS) for the test set which represents the total prediction error of the model on unseen data\n",
    "3. Line3: Computes the total sum of squares (TSS) for the test set which represents the total variance in the target values\n",
    "4. Line4: Calculates the pseudo r2 score used to evaluate the model performance on the test data by measuring how much variability in the test target data is explained by the model"
   ],
   "id": "282543cb9906890b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logging Neural Network Results into the Model Comparison Report",
   "id": "a405d2dc45d8ac82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DataPrep.report.loc[len(DataPrep.report)] = [\"NN_LE \", r2, pseudor2,\"\", CV_nnmodel.cv_results_['mean_test_score'][CV_nnmodel.best_index_], CV_nnmodel.cv_results_['std_test_score'][CV_nnmodel.best_index_]]",
   "id": "1fe1964d3e32f46c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line adds a new row to the shared 'DataPrep.report' table documenting the performance metrics of the neural network model trained with label-encoded data.",
   "id": "a94a8537c168afb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Displaying the best hyperparameters",
   "id": "f1f0e606d77d97ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(CV_nnmodel.best_params_)",
   "id": "4e614a07b61fc123"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line prints the optimal hyperparameter configuration found by 'GridSearchCV' during training of the MLPRegressor",
   "id": "b34bc2d2080cfe3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Splitting One-Hot Encoded Data for Neural Network Training and Testing",
   "id": "b3407faee773928f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "(X_train_nn, X_test_nn, Y_train_nn, Y_test_nn) = train_test_split(DataPrep.X_OH, DataPrep.Y_OH, test_size=0.2, random_state=42)",
   "id": "64a2248bdbb547a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This line splits the dataset - specifically the one-hot encoded version - into a training set and a test set preparing it for use with a neural network regression model",
   "id": "956ceebb7229c03a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Grid Search to Train and Tune a Neural Network with One-Hot Encoded Data",
   "id": "4479c47c69537092"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CV_nnmodel = GridSearchCV(estimator=NNetRregCV, param_grid=param_grid, cv=10,n_jobs=-1)\n",
    "CV_nnmodel.fit(X_train_nn, Y_train_nn)"
   ],
   "id": "e5f28dc6e584db78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code performs automated hyperparameter tuning using 'GridSearchCV' for a neural network regression model trained on one-hot encoded features.\n",
    "1. Line1: Wraps the 'MLPRegressor' in a 'GridSearchCV' object to perform an exhaustive search over a grid of hyperparameters\n",
    "2. Line2: Trains the model using the training data"
   ],
   "id": "55bbdcd1c68e8048"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculating r2 score for Neural Network",
   "id": "63d38bee48c6a753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_train_pred = CV_nnmodel.predict(X_train_nn)\n",
    "Y_train_dev = sum((Y_train_nn - Y_train_pred)**2)\n",
    "Y_train_meandev = sum((Y_train_nn - Y_train_nn.mean())**2)  # [aus PDF]\n",
    "r2 = 1 - Y_train_dev / Y_train_meandev  # [aus PDF]"
   ],
   "id": "95ed8db63e1c684a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This block manually computes the r2 score for the neural network model trained on one-hot encoded features using the training dataset\n",
    "1. Line1: Predicts the target values on the training data using the best neural network selected by 'GridSearchCV'\n",
    "2. Line2: Calculates the residual sum of squares (RSS) which represents the total prediction error of the model on the training set.\n",
    "3. Line3: Calculates the total sum of squares (TSS) which represents the total variance in the training data assuming a baseline model that always predicts the mean\n",
    "4. Line4: Computes the r2 score which measures how well the model explains the variance in the target variable"
   ],
   "id": "5a5dbf2ec05b102"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculating pseudo r2 score for Neural Network",
   "id": "3f6b5a14f226fcef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Y_test_pred = CV_nnmodel.predict(X_test_nn)\n",
    "Y_test_dev = sum((Y_test_nn - Y_test_pred)**2)\n",
    "Y_train_meandev = sum((Y_test_nn - Y_test_nn.mean())**2)  # [aus PDF]\n",
    "pseudor2 = 1 - Y_test_dev / Y_train_meandev"
   ],
   "id": "92b4026a5481cdce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code block calculates the r2 score on the test dataset (pseudo r2) to evaluate the generalization performance of the neural network trained on one-hot encoded features.\n",
    "1. Line1: Uses the trained neural network to predict values for the test set.\n",
    "2. Line2: Computes the residual sum of squares (RSS) on the test set and measures the total prediction error made by the model on unseen data\n",
    "3. Line3: Calculates the total sum of squares (TSS) on the test data\n",
    "4. Line4: Calculates the test r2 (pseudo r2) which reflects how well the model generalizes to new data"
   ],
   "id": "262c5f1a40d842e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logging Neural Network Results to the Report",
   "id": "fbee756d7fa9f719"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DataPrep.report.loc[len(DataPrep.report)] = [\"NN_OH \", r2, pseudor2,\"\", CV_nnmodel.cv_results_['mean_test_score'][CV_nnmodel.best_index_], CV_nnmodel.cv_results_['std_test_score'][CV_nnmodel.best_index_]]\n",
    "print(DataPrep.report.head())\n",
    "print(CV_nnmodel.best_params_)"
   ],
   "id": "6060fd2bbc6b7b78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This block adds a new row to your 'DataPrep.report' table to document the performance of a neural network model trained on one-hot encoded features and then displays the first few rows of the report and the best hyperparameters found during grid search.",
   "id": "f834bb30c70893e5"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
